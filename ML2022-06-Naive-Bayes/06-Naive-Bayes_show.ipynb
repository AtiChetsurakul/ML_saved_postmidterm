{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"ati tesakulsiri\"\n",
    "ID = \"st123009\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 06: Generative classifiers: Naive Bayes\n",
    "\n",
    "As discussed in class, a Naive Bayes classifier works as follows:\n",
    "$$\\begin{eqnarray}\n",
    "p(y \\mid \\mathbf{x} ; \\theta) & = & \\frac{p(\\mathbf{x} \\mid y ; \\theta) p(y ; \\theta)}{p(\\mathbf{x} ; \\theta)} \\\\\n",
    "& \\propto & p(\\mathbf{x} \\mid y ; \\theta) p(y ; \\theta) \\\\\n",
    "& \\approx & p(y ; \\theta) \\prod_j p(x_j \\mid y ; \\theta)\n",
    "\\end{eqnarray}$$\n",
    "We will use Naive Bayes to perform diabetes diagnosis and text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Diabetes classification\n",
    "\n",
    "In this example we predict wheter a patient with specific diagnostic measurements has diabetes or not. As the features are\n",
    "continuous, we will model the conditional probabilities\n",
    "$p(x_j \\mid y ; \\theta)$ as univariate Gaussians with mean $\\mu_{j,y}$ and standard deviation $\\sigma_{j,y}$.\n",
    "\n",
    "The data are originally from the U.S. National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK) and are available\n",
    "from [Kaggle](https://www.kaggle.com/uciml/pima-indians-diabetes-database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation\n",
    "\n",
    "First we have some functions to read the dataset, split it into train and test, and partition it according to target class ($y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "def loadCsv(filename):\n",
    "    data_raw = pd.read_csv(filename)\n",
    "    headers = data_raw.columns\n",
    "    dataset = data_raw.values\n",
    "    return dataset, headers\n",
    "\n",
    "# Split dataset into test and train with given ratio\n",
    "def splitDataset(test_size,*arrays,**kwargs):\n",
    "    return train_test_split(*arrays,test_size=test_size,**kwargs)\n",
    "\n",
    "# Separate training data according to target class\n",
    "# Return key value pairs array in which keys are possible target variable values\n",
    "# and values are the data records.\n",
    "\n",
    "def data_split_byClass(dataset):\n",
    "    Xy = {}\n",
    "    for i in range(len(dataset)):\n",
    "        datapair = dataset[i]\n",
    "        # datapair[-1] (the last column) is the target class for this record.\n",
    "        # Check if we already have this value as a key in the return array\n",
    "        if (datapair[-1] not in Xy):\n",
    "            # Add class as key\n",
    "            Xy[datapair[-1]] = []\n",
    "        # Append this record to array of records for this class key\n",
    "        Xy[datapair[-1]].append(datapair)\n",
    "    return Xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "Next we have some functions used for training the model. Parameters include mean and standard deviation, used\n",
    "to partition numerical variables into categorical variables, as well as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of a Gaussian are its mean and standard deviation\n",
    "\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    "\n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([pow(x-avg,2) for x in numbers])/float(len(numbers)-1)\n",
    "    return math.sqrt(variance)\n",
    "\n",
    "# Calculate Gaussian parameters mu and sigma for each attribute over a dataset\n",
    "\n",
    "def get_gaussian_parameters(X,y):\n",
    "    parameters = {}\n",
    "    unique_y = np.unique(y)\n",
    "    for uy in unique_y:\n",
    "        mean = np.mean(X[y==uy],axis=0)\n",
    "        std = np.std(X[y==uy],axis=0)\n",
    "        py = y[y==uy].size/y.size\n",
    "        parameters[uy] = {'prior':py,'mean':mean,'std':std}\n",
    "    return parameters, unique_y\n",
    "\n",
    "def calculateProbability(x, mu, sigma):\n",
    "    sigma = np.diag(sigma**2)\n",
    "    x = x.reshape(-1,1)\n",
    "    mu = mu.reshape(-1,1)\n",
    "    exponent = np.exp(-1/2*(x-mu).T@np.linalg.inv(sigma)@(x-mu))\n",
    "    return ((1/(np.sqrt(((2*np.pi)**x.size)*np.linalg.det(sigma))))*exponent)[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing\n",
    "\n",
    "Next some functions for testing the model on a test set and computing its accuracy. Note that we assume\n",
    "$$ p(y \\mid \\mathbf{x} ; \\theta) \\propto p(\\mathbf{x} \\mid y ; \\theta), $$\n",
    "which means we assume that the priors $p(y)$ are equal for each possible value of $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class conditional probabilities for given input data vector\n",
    "\n",
    "def predict_one(x,parameters,unique_y,prior = True):\n",
    "    probabilities = []\n",
    "    for key in parameters.keys():\n",
    "        probabilities.append(calculateProbability(x,parameters[key]['mean'],parameters[key]['std'])*(parameters[key]['prior']**(float(prior))))\n",
    "    probabilities = np.array(probabilities)\n",
    "    return unique_y[np.argmax(probabilities)]\n",
    "\n",
    "def getPredictions(X, parameters, unique_y,prior=True):\n",
    "    predictions = []\n",
    "    for i in range(X.shape[0]):\n",
    "        predictions.append(predict_one(X[i],parameters,unique_y,prior))\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Get accuracy for test set\n",
    "\n",
    "def getAccuracy(y, y_pred):\n",
    "    correct = len(y[y==y_pred])\n",
    "    return correct/y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "Here we load the diabetes dataset, split it into training and test data, train a Gaussian NB model, and test the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total = 768 Train = 460 Test = 308\n",
      "Accuracy with Prior = 0.7272727272727273\n",
      "Accuracy without Prior = 0.7337662337662337\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "filename = 'diabetes.csv'\n",
    "dataset, headers = loadCsv(filename)\n",
    "#print(headers)\n",
    "#print(np.array(dataset)[0:5,:])\n",
    "\n",
    "# Split into training and test\n",
    "\n",
    "X_train,X_test,y_train,y_test = splitDataset(0.4,dataset[:,:-1],dataset[:,-1])\n",
    "print(\"Total =\",len(dataset),\"Train =\", len(X_train),\"Test =\",len(X_test))\n",
    "\n",
    "# Train model\n",
    "\n",
    "parameters, unique_y = get_gaussian_parameters(X_train,y_train)\n",
    "prediction = getPredictions(X_test,parameters,unique_y)\n",
    "print(\"Accuracy with Prior =\",getAccuracy(y_test,prediction))\n",
    "\n",
    "# Test model\n",
    "\n",
    "prediction = getPredictions(X_test,parameters,unique_y,prior = False)\n",
    "print(\"Accuracy without Prior =\",getAccuracy(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0bcbc8407fdfb3f7d96cc07ad4d3124",
     "grade": false,
     "grade_id": "cell-9a740adb2ea13611",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "###  Exercise In lab / take home work (20 points)\n",
    "\n",
    "Find out the proportion of the records in your dataset are positive vs. negative.  Can we conclude that $p(y=1) = p(y=0)$? If not, add\n",
    "the priors $p(y=1)$ and $p(y=0)$ to your NB model. Does it improve the result?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cf1a770bb1ee9faa6961f202769b10e",
     "grade": true,
     "grade_id": "cell-c1a5f74a2d330b0c",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(y=1) = 0.34347826086956523 \n",
      "and p(y=0) = 0.6565217391304348\n",
      "\n",
      "Accuracy with Prior = 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "py1 = y_train[y_train == 1].shape[0]/y_train.shape[0]\n",
    "\n",
    "py0 = y_train[y_train == 0].shape[0]/y_train.shape[0]\n",
    "py1,py0\n",
    "\n",
    "prediction = getPredictions(X_test,parameters,unique_y,prior = True)\n",
    "print(f'p(y=1) = {py1} \\nand p(y=0) = {py0}\\n')\n",
    "print(\"Accuracy with Prior =\",getAccuracy(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f416d2fceb4d17b1865da03585d6f44f",
     "grade": false,
     "grade_id": "cell-a32adf75650dfd55",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Explain that you can conclude that $p(y=1) = p(y=0)$? If not, add\n",
    "the priors $p(y=1)$ and $p(y=0)$ to your NB model. Does it improve the result? (double click to explain)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## ANS\n",
    "\n",
    "- p(y=1) = p(y=0)?\n",
    "    - No, in this dataset the p(y=1) != p(y=0) as describe in above cell.\n",
    "    \n",
    "- adding prior and compare\n",
    "    - testing in above cell.\n",
    "\n",
    "\n",
    "- Does it improve the result?\n",
    "    - I have not seen any difference in this dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Text classification\n",
    "\n",
    "This example has been adapted from a post by Jaya Aiyappan, available at\n",
    "[Analytics Vidhya](https://medium.com/analytics-vidhya/naive-bayes-classifier-for-text-classification-556fabaf252b#:~:text=The%20Naive%20Bayes%20classifier%20is,time%20and%20less%20training%20data).\n",
    "\n",
    "We will generate a small dataset of sentences that are classified as either \"statements\" or \"questions.\"\n",
    "\n",
    "We will assume that occurance and placement of words within a sentence is independent of each other\n",
    "(i.e., the features are conditionally independent given $y$). So the sentence \"this is my book\" is the same as \"is this my book.\"\n",
    "We will treat words as case insensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             sentence      class\n",
      "0               This is my novel book  statement\n",
      "1  this book has more than one author  statement\n",
      "2                     is this my book   question\n",
      "3                     They are novels  statement\n",
      "4             have you read this book   question\n",
      "5            who is the novels author   question\n",
      "6             what are the characters   question\n",
      "7       This is how I bought the book  statement\n",
      "8         I like fictional characters  statement\n",
      "9          what is your favorite book   question\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "                        sentence      class\n",
      "0               this is the book  statement\n",
      "1  who are the novels characters   question\n",
      "2             is this the author   question\n",
      "3                  I like apples       None\n"
     ]
    }
   ],
   "source": [
    "# Generate text data for two classes, \"statement\" and \"question\"\n",
    "\n",
    "text_train = [['This is my novel book', 'statement'],\n",
    "              ['this book has more than one author', 'statement'],\n",
    "              ['is this my book', 'question'],\n",
    "              ['They are novels', 'statement'],\n",
    "              ['have you read this book', 'question'],\n",
    "              ['who is the novels author', 'question'],\n",
    "              ['what are the characters', 'question'],\n",
    "              ['This is how I bought the book', 'statement'],\n",
    "              ['I like fictional characters', 'statement'],\n",
    "              ['what is your favorite book', 'question']]\n",
    "\n",
    "text_test = [['this is the book', 'statement'], \n",
    "             ['who are the novels characters', 'question'], \n",
    "             ['is this the author', 'question'],\n",
    "            ['I like apples']]\n",
    "\n",
    "# Load training and test data into pandas data frames\n",
    "\n",
    "training_data = pd.DataFrame(text_train, columns= ['sentence', 'class'])\n",
    "print(training_data)\n",
    "print('\\n------------------------------------------\\n')\n",
    "testing_data = pd.DataFrame(text_test, columns= ['sentence', 'class'])\n",
    "print(testing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition training data by class\n",
    "\n",
    "stmt_docs = [train['sentence'] for index,train in training_data.iterrows() if train['class'] == 'statement']\n",
    "question_docs = [train['sentence'] for index,train in training_data.iterrows() if train['class'] == 'question']\n",
    "all_docs = [train['sentence'] for index,train in training_data.iterrows()]\n",
    "\n",
    "# Get word frequencies for each sentence and class\n",
    "\n",
    "def get_words(text):\n",
    "    # Initialize word list\n",
    "    words = [];\n",
    "    # Loop through each sentence in input array\n",
    "    for text_row in text:       \n",
    "        # Check the number of words. Assume each word is separated by a blank space\n",
    "        # so that the number of words is the number of blank spaces + 1\n",
    "        number_of_spaces = text_row.count(' ')\n",
    "        # loop through the sentence and get words between blank spaces.\n",
    "        for i in range(number_of_spaces):\n",
    "            # Check for for last word\n",
    "            words.append([text_row[:text_row.index(' ')].lower()])\n",
    "            text_row = text_row[text_row.index(' ')+1:]  \n",
    "            i = i + 1        \n",
    "        words.append([text_row])\n",
    "    return np.unique(words)\n",
    "\n",
    "# Get frequency of each word in each document\n",
    "\n",
    "def get_doc_word_frequency(words, text):  \n",
    "    word_freq_table = np.zeros((len(text),len(words)), dtype=int)\n",
    "    i = 0\n",
    "    for text_row in text:\n",
    "        # Insert extra space between each pair of words to prevent\n",
    "        # partial match of words\n",
    "        text_row_temp = ''\n",
    "        for idx, val in enumerate(text_row):\n",
    "            if val == ' ':\n",
    "                 text_row_temp = text_row_temp + '  '\n",
    "            else:\n",
    "                  text_row_temp = text_row_temp + val.lower()\n",
    "        text_row = ' ' + text_row_temp + ' '\n",
    "        j = 0\n",
    "        for word in words: \n",
    "            word = ' ' + word + ' '\n",
    "            freq = text_row.count(word)\n",
    "            word_freq_table[i,j] = freq\n",
    "            j = j + 1\n",
    "        i = i + 1\n",
    "    \n",
    "    return word_freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   are  author  book  bought  characters  fictional  has  how  i  is  like  \\\n",
      "0    0       0     1       0           0          0    0    0  0   1     0   \n",
      "1    0       1     1       0           0          0    1    0  0   0     0   \n",
      "2    1       0     0       0           0          0    0    0  0   0     0   \n",
      "3    0       0     1       1           0          0    0    1  1   1     0   \n",
      "4    0       0     0       0           1          1    0    0  1   0     1   \n",
      "\n",
      "   more  my  novel  novels  one  than  the  they  this  \n",
      "0     0   1      1       0    0     0    0     0     1  \n",
      "1     1   0      0       0    1     1    0     0     1  \n",
      "2     0   0      0       1    0     0    0     1     0  \n",
      "3     0   0      0       0    0     0    1     0     1  \n",
      "4     0   0      0       0    0     0    0     0     0  \n"
     ]
    }
   ],
   "source": [
    "# Get word frequencies for statement documents\n",
    "\n",
    "word_list_s = get_words(stmt_docs)\n",
    "word_freq_table_s = get_doc_word_frequency(word_list_s, stmt_docs)\n",
    "tdm_s = pd.DataFrame(word_freq_table_s, columns=word_list_s)\n",
    "print(tdm_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'are': 1, 'author': 1, 'book': 3, 'bought': 1, 'characters': 1, 'fictional': 1, 'has': 1, 'how': 1, 'i': 2, 'is': 2, 'like': 1, 'more': 1, 'my': 1, 'novel': 1, 'novels': 1, 'one': 1, 'than': 1, 'the': 1, 'they': 1, 'this': 3}\n"
     ]
    }
   ],
   "source": [
    "# Get word frequencies over all statement documents\n",
    "\n",
    "freq_list_s = word_freq_table_s.sum(axis=0) \n",
    "freq_s = dict(zip(word_list_s,freq_list_s))\n",
    "print(freq_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   are  author  book  characters  favorite  have  is  my  novels  read  the  \\\n",
      "0    0       0     1           0         0     0   1   1       0     0    0   \n",
      "1    0       0     1           0         0     1   0   0       0     1    0   \n",
      "2    0       1     0           0         0     0   1   0       1     0    1   \n",
      "3    1       0     0           1         0     0   0   0       0     0    1   \n",
      "4    0       0     1           0         1     0   1   0       0     0    0   \n",
      "\n",
      "   this  what  who  you  your  \n",
      "0     1     0    0    0     0  \n",
      "1     1     0    0    1     0  \n",
      "2     0     0    1    0     0  \n",
      "3     0     1    0    0     0  \n",
      "4     0     1    0    0     1  \n"
     ]
    }
   ],
   "source": [
    "# Get word frequencies for question documents\n",
    "\n",
    "word_list_q = get_words(question_docs)\n",
    "word_freq_table_q = get_doc_word_frequency(word_list_q, question_docs)\n",
    "tdm_q = pd.DataFrame(word_freq_table_q, columns=word_list_q)\n",
    "print(tdm_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'are': 1, 'author': 1, 'book': 3, 'characters': 1, 'favorite': 1, 'have': 1, 'is': 3, 'my': 1, 'novels': 1, 'read': 1, 'the': 2, 'this': 2, 'what': 2, 'who': 1, 'you': 1, 'your': 1}\n",
      "[1 1 3 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 3]\n",
      "[1 1 3 1 1 1 3 1 1 1 2 2 2 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Get word frequencies over all question documents\n",
    "\n",
    "freq_list_q = word_freq_table_q.sum(axis=0) \n",
    "freq_q = dict(zip(word_list_q,freq_list_q))\n",
    "print(freq_q)\n",
    "print(freq_list_s)\n",
    "print(freq_list_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of words for \"statement\" class \n",
      "\n",
      "{'are': 0.043478260869565216, 'author': 0.043478260869565216, 'book': 0.08695652173913043, 'bought': 0.043478260869565216, 'characters': 0.043478260869565216, 'fictional': 0.043478260869565216, 'has': 0.043478260869565216, 'how': 0.043478260869565216, 'i': 0.06521739130434782, 'is': 0.06521739130434782, 'like': 0.043478260869565216, 'more': 0.043478260869565216, 'my': 0.043478260869565216, 'novel': 0.043478260869565216, 'novels': 0.043478260869565216, 'one': 0.043478260869565216, 'than': 0.043478260869565216, 'the': 0.043478260869565216, 'they': 0.043478260869565216, 'this': 0.08695652173913043}\n",
      "------------------------------------------- \n",
      "\n",
      "Probability of words for \"question\" class \n",
      "\n",
      "{'are': 0.05128205128205128, 'author': 0.05128205128205128, 'book': 0.10256410256410256, 'characters': 0.05128205128205128, 'favorite': 0.05128205128205128, 'have': 0.05128205128205128, 'is': 0.10256410256410256, 'my': 0.05128205128205128, 'novels': 0.05128205128205128, 'read': 0.05128205128205128, 'the': 0.07692307692307693, 'this': 0.07692307692307693, 'what': 0.07692307692307693, 'who': 0.05128205128205128, 'you': 0.05128205128205128, 'your': 0.05128205128205128}\n"
     ]
    }
   ],
   "source": [
    "# Get word probabilities for statement class\n",
    "a = 1\n",
    "prob_s = []\n",
    "for count in freq_list_s:\n",
    "    #print(word, count)\n",
    "    prob_s.append((count+a)/(sum(freq_list_s)+len(freq_list_s)*a))\n",
    "prob_s.append(a/(sum(freq_list_s)+len(freq_list_s)*a))\n",
    "    \n",
    "# Get word probabilities for question class\n",
    "\n",
    "prob_q = []\n",
    "for count in freq_list_q:\n",
    "    prob_q.append((count+a)/(sum(freq_list_q)+len(freq_list_q)*a))\n",
    "prob_q.append(a/(sum(freq_list_q)+len(freq_list_q)*a))   \n",
    "    \n",
    "    \n",
    "print('Probability of words for \"statement\" class \\n')\n",
    "print(dict(zip(word_list_s, prob_s)))\n",
    "print('------------------------------------------- \\n')\n",
    "print('Probability of words for \"question\" class \\n')\n",
    "print(dict(zip(word_list_q, prob_q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prior for one class\n",
    "\n",
    "def prior(className):    \n",
    "    denominator = len(stmt_docs) + len(question_docs)\n",
    "    \n",
    "    if className == 'statement':\n",
    "        numerator =  len(stmt_docs)\n",
    "    else:\n",
    "        numerator =  len(question_docs)\n",
    "        \n",
    "    return np.divide(numerator,denominator)\n",
    "    \n",
    "# Calculate class conditional probability for a sentence\n",
    "    \n",
    "def classCondProb(sentence, className):\n",
    "    words = get_words(sentence)\n",
    "    prob = 1\n",
    "    for word in words:\n",
    "        if className == 'statement':\n",
    "            idx = np.where(word_list_s == word)\n",
    "            prob = prob * prob_s[np.array(idx)[0,0]]\n",
    "        else:\n",
    "            idx = np.where(word_list_q == word)\n",
    "            prob = prob * prob_q[np.array(idx)[0,0]]   \n",
    "    \n",
    "    return prob\n",
    "\n",
    "# Predict class of a sentence\n",
    "\n",
    "def predict(sentence):\n",
    "    prob_statement = classCondProb(sentence, 'statement') * prior('statement')\n",
    "    prob_question = classCondProb(sentence, 'question') * prior('question')\n",
    "    if  prob_statement > prob_question:\n",
    "        return 'statement'\n",
    "    else:\n",
    "        return 'question'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06f31d4f4657c9d7a61ca287b3a51c86",
     "grade": false,
     "grade_id": "cell-3b166fac02ec4711",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### In-lab exercise: Laplace smoothing\n",
    "\n",
    "Run the code below and figure out why it fails.\n",
    "\n",
    "When a word does not appear with a specific class in the training data, its class-conditional probability is 0, and we are unable to\n",
    "get a reasonable probability for that class.\n",
    "\n",
    "Research Laplace smoothing, and modify the code above to implement Laplace smoothing (setting the frequency of all words with frequency 0 to a frequency of 1).\n",
    "Run the modified code on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting prediction for \"this is the book\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_114/746921951.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtesting_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Getting prediction for \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtest_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_114/293703453.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mprob_statement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassCondProb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'statement'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'statement'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mprob_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassCondProb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'question'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m  \u001b[0mprob_statement\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mprob_question\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_114/293703453.py\u001b[0m in \u001b[0;36mclassCondProb\u001b[0;34m(sentence, className)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclassName\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'statement'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list_s\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprob_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list_q\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 1 with size 0"
     ]
    }
   ],
   "source": [
    "test_docs = list([test['sentence'] for index,test in testing_data.iterrows()])\n",
    "print('Getting prediction for \"%s\"' % test_docs[0])\n",
    "predict(test_docs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c92cc1750ff6519f167950df416dbad7",
     "grade": false,
     "grade_id": "cell-9d86c9d269d1a550",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.1 (10 points)\n",
    "\n",
    "Explain Why it failed and explain how to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a8b94cb831bb4a934e82d63cdf77be3",
     "grade": false,
     "grade_id": "cell-d424d31d1e17fd89",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Explanation here! (Double click to explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The word using in the test set is a new, undiscover or not in the model, so when we get to thier index with blank it raise an error.\n",
    "<br>\n",
    "## to fix this we need to fix the `classCondProb function` to multiply with p(y;) instead here the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a281f0b0b151358f4dbbcbfb6adbd38",
     "grade": false,
     "grade_id": "cell-e17217b48752c1fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.2 (20 points)\n",
    "\n",
    "Modify your code and make it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e51fedc8aefb2614a7936134d6333f67",
     "grade": false,
     "grade_id": "cell-0722e2f212d3751c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "# idx = np.where(word_list_s == 'author')\n",
    "# if 'author' not in word_list_s:\n",
    "#     print('hi')\n",
    "# prob_s[np.array(idx)[0,0]]\n",
    "\n",
    "def classCondProb(sentence, className):\n",
    "    words = get_words(sentence)\n",
    "    prob = 1\n",
    "    for word in words:\n",
    "        if className == 'statement':\n",
    "            if word not in word_list_s:\n",
    "                # print('hi')\n",
    "                prob = prob * prior(className)\n",
    "            else:\n",
    "                idx = np.where(word_list_s == word)\n",
    "                prob = prob * prob_s[np.array(idx)[0,0]]\n",
    "        else:\n",
    "            if word not in word_list_q:\n",
    "                prob = prob * prior(className)\n",
    "            else:\n",
    "                idx = np.where(word_list_q == word)\n",
    "                print(idx)\n",
    "                prob = prob * prob_q[np.array(idx)[0,0]]   \n",
    "    \n",
    "    return prob\n",
    "\n",
    "# classCondProb('this is the book', 'question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b12f4c8d6b4163d2f6cfe464b3ba62f3",
     "grade": true,
     "grade_id": "cell-c576e7ed4dd3046a",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting prediction for this is the book\"\n",
      "question\n",
      "Getting prediction for who are the novels characters\"\n",
      "question\n",
      "Getting prediction for is this the author\"\n",
      "question\n",
      "Getting prediction for I like apples\"\n",
      "question\n",
      "success!\n"
     ]
    }
   ],
   "source": [
    "# Test function: Do not remove\n",
    "test_docs = list([test['sentence'] for index,test in testing_data.iterrows()])\n",
    "\n",
    "for sentence in test_docs:\n",
    "    print('Getting prediction for %s\"' % sentence)\n",
    "    print(predict(sentence))\n",
    "    \n",
    "print(\"success!\")\n",
    "# End Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63a751b5f418dd7b04bf156032e8435f",
     "grade": false,
     "grade_id": "cell-12db07859804f68d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expect result**:\\\n",
    "Getting prediction for this is the book\"\\\n",
    "question\\\n",
    "Getting prediction for who are the novels characters\"\\\n",
    "question\\\n",
    "Getting prediction for is this the author\"\\\n",
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a3302658ee7212f23942b2d4d93e70c",
     "grade": false,
     "grade_id": "cell-2bc1a154cce1dd7a",
     "locked": true,
     "points": 50,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Take home exercise\n",
    "\n",
    "Find a more substantial text classification dataset, clean up the documents, and build your NB classifier. Write a brief report on your in-lab and take home exercises and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
 
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv('HIDE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category    0\n",
       "Message     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.describe()\n",
    "data_set_cut = data_set[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Category', 'Message'], dtype='object')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_cut.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atdfhsgjklyy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114/2151206352.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_set_cut['Message'] = data_set_cut['Message'].apply(cleaner)\n"
     ]
    }
   ],
   "source": [
    "# data_set_cut = data_set_cut.reset_index()\n",
    "cleaner = FUNCTION HIDE",
    "\n",
    "test_sen = 'atdfhsgjk;l43895y68875y8**&*(%^&*('\n",
    "print(cleaner(test_sen))\n",
    "\n",
    "data_set_cut['Message'] = data_set_cut['Message'].apply(cleaner) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def splitDataset(test_size,*arrays,**kwargs):\n",
    "#     return train_test_split(*arrays,test_size=test_size,**kwargs)\n",
    "# X_train,X_test,y_train,y_test = splitDataset(0.6,data_set_cut.Message,data_set_cut.Category)\n",
    "train_data = data_set_cut.iloc[:200]\n",
    "test_data = data_set_cut.iloc[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in  a wkly comp to win FA Cup final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point crazy Available only in ...\n",
       "1      ham                            Ok lar Joking wif u oni\n",
       "2     spam  Free entry in  a wkly comp to win FA Cup final...\n",
       "3      ham        U dun say so early hor U c already then say\n",
       "4      ham  Nah I dont think he goes to usf he lives aroun..."
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_docs = [train['Message'] for index,train in train_data.iterrows() if train['Category'] == 'ham']\n",
    "spam_docs = [train['Message'] for index,train in train_data.iterrows() if train['Category'] == 'spam']\n",
    "all_docs = [train['Message'] for index,train in train_data.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    # Initialize word list\n",
    "    words = [];\n",
    "    # Loop through each sentence in input array\n",
    "    for text_row in text:       \n",
    "        # Check the number of words. Assume each word is separated by a blank space\n",
    "        # so that the number of words is the number of blank spaces + 1\n",
    "        number_of_spaces = text_row.count(' ')\n",
    "        # loop through the sentence and get words between blank spaces.\n",
    "        for i in range(number_of_spaces):\n",
    "            # Check for for last word\n",
    "            words.append([text_row[:text_row.index(' ')].lower()])\n",
    "            text_row = text_row[text_row.index(' ')+1:]  \n",
    "            i = i + 1        \n",
    "        words.append([text_row])\n",
    "    return np.unique(words)\n",
    "\n",
    "# Get frequency of each word in each document\n",
    "\n",
    "def get_doc_word_frequency(words, text):  \n",
    "    word_freq_table = np.zeros((len(text),len(words)), dtype=int)\n",
    "    i = 0\n",
    "    for text_row in text:\n",
    "        # Insert extra space between each pair of words to prevent\n",
    "        # partial match of words\n",
    "        text_row_temp = ''\n",
    "        for idx, val in enumerate(text_row):\n",
    "            if val == ' ':\n",
    "                 text_row_temp = text_row_temp + '  '\n",
    "            else:\n",
    "                  text_row_temp = text_row_temp + val.lower()\n",
    "        text_row = ' ' + text_row_temp + ' '\n",
    "        j = 0\n",
    "        for word in words: \n",
    "            word = ' ' + word + ' '\n",
    "            freq = text_row.count(word)\n",
    "            word_freq_table[i,j] = freq\n",
    "            j = j + 1\n",
    "        i = i + 1\n",
    "    \n",
    "    return word_freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Abiola  Callertune  Hee  IQ  LUCYxx  Lol  No  SEEING  Smiling  ...  \\\n",
      "0    19       0           0    0   0       0    0   0       0        0  ...   \n",
      "1     5       0           0    0   0       0    0   0       0        0  ...   \n",
      "2    10       0           0    0   0       0    0   0       0        0  ...   \n",
      "3    12       0           0    0   0       0    0   0       0        0  ...   \n",
      "4    15       0           0    0   0       0    0   0       0        0  ...   \n",
      "..   ..     ...         ...  ...  ..     ...  ...  ..     ...      ...  ...   \n",
      "162  18       0           0    0   0       0    0   0       0        0  ...   \n",
      "163   6       0           0    0   0       0    0   0       0        0  ...   \n",
      "164   5       0           0    0   0       0    0   0       0        0  ...   \n",
      "165  13       0           0    0   0       0    0   0       0        0  ...   \n",
      "166  29       0           0    0   0       0    0   0       0        0  ...   \n",
      "\n",
      "     yo  you  youd  youhow  youll  your  youre  yourself  youve  yup  \n",
      "0     0    0     0       0      0     0      0         0      0    0  \n",
      "1     0    0     0       0      0     0      0         0      0    0  \n",
      "2     0    0     0       0      0     0      0         0      0    0  \n",
      "3     0    0     0       0      0     0      0         0      0    0  \n",
      "4     0    0     0       0      0     0      0         0      0    0  \n",
      "..   ..  ...   ...     ...    ...   ...    ...       ...    ...  ...  \n",
      "162   0    2     0       0      0     0      0         0      1    0  \n",
      "163   0    0     0       0      0     0      0         0      0    0  \n",
      "164   0    0     0       0      0     0      0         0      0    0  \n",
      "165   0    1     0       0      0     1      0         0      0    0  \n",
      "166   0    1     0       0      0     1      0         0      0    0  \n",
      "\n",
      "[167 rows x 886 columns]\n"
     ]
    }
   ],
   "source": [
    "word_list_ham = get_words(ham_docs)\n",
    "word_freq_table_ham = get_doc_word_frequency(word_list_ham, ham_docs)\n",
    "tdm_ham = pd.DataFrame(word_freq_table_ham, columns=word_list_ham)\n",
    "print(tdm_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 2376, 'Abiola': 0, 'Callertune': 0, 'Hee': 0, 'IQ': 0, 'LUCYxx': 0, 'Lol': 0, 'No': 0, 'SEEING': 0, 'Smiling': 0, 'Sorry': 0, 'WILL': 0, 'XX': 0, 'You': 0, 'Yummy': 0, 'a': 40, 'aaooooright': 1, 'able': 1, 'about': 5, 'abt': 2, 'accomodate': 1, 'accomodations': 1, 'account': 1, 'actin': 1, 'activities': 1, 'address': 3, 'aft': 1, 'after': 4, 'afternoon': 2, 'again': 3, 'ah': 1, 'ahead': 2, 'ahhh': 1, 'aids': 1, 'aight': 1, 'all': 16, 'almost': 1, 'already': 8, 'alright': 1, 'also': 1, 'always': 3, 'am': 11, 'amore': 1, 'amp': 1, 'ams': 1, 'an': 2, 'and': 37, 'animation': 1, 'another': 1, 'answer': 1, 'any': 1, 'anymore': 2, 'anythin': 1, 'anything': 7, 'anyway': 1, 'anyways': 1, 'apartment': 1, 'apologetic': 1, 'apologise': 1, 'applespairsall': 1, 'appointment': 1, 'approaches': 1, 'arabian': 1, 'ard': 2, 'are': 19, 'around': 2, 'as': 9, 'ask': 1, 'askd': 1, 'at': 13, 'available': 1, 'ave': 1, 'avoid': 1, 'awesome': 1, 'axis': 1, 'b': 3, 'babe': 2, 'babyjontet': 1, 'back': 7, 'badly': 1, 'bak': 1, 'bank': 2, 'bath': 1, 'bathe': 1, 'bday': 1, 'be': 9, 'because': 1, 'become': 1, 'becoz': 1, 'bed': 1, 'been': 7, 'befor': 1, 'beforehand': 1, 'begin': 1, 'begins': 1, 'being': 3, 'believe': 1, 'beloved': 1, 'best': 2, 'better': 2, 'biggest': 1, 'birla': 1, 'birthday': 2, 'bit': 3, 'blessing': 1, 'bloody': 2, 'boss': 1, 'boston': 2, 'bother': 1, 'boy': 1, 'boytoy': 1, 'breather': 1, 'bridge': 1, 'brother': 1, 'buffet': 1, 'bugis': 1, 'burger': 1, 'bus': 1, 'busy': 1, 'but': 13, 'buy': 3, 'buying': 1, 'by': 2, 'c': 3, 'cabin': 1, 'call': 12, 'callers': 3, 'callertune': 6, 'calls': 1, 'callsmessagesmissed': 1, 'came': 1, 'can': 11, 'cant': 3, 'car': 4, 'carlos': 1, 'cash': 2, 'casualty': 1, 'catch': 2, 'caught': 1, 'cause': 1, 'cave': 1, 'changed': 2, 'check': 4, 'checking': 1, 'cheers': 1, 'child': 1, 'chores': 1, 'cine': 2, 'class': 3, 'clear': 1, 'close': 2, 'clue': 1, 'coffee': 1, 'coins': 1, 'come': 2, 'comes': 1, 'comin': 1, 'coming': 1, 'completed': 1, 'completely': 1, 'computer': 1, 'considering': 1, 'contact': 1, 'convincing': 1, 'cool': 1, 'copy': 3, 'cos': 3, 'cost': 2, 'could': 2, 'course': 1, 'cozsomtimes': 1, 'crashing': 1, 'crave': 1, 'crazy': 1, 'cried': 1, 'cultures': 1, 'cup': 1, 'cuppa': 1, 'cut': 2, 'cuz': 1, 'd': 1, 'da': 2, 'date': 1, 'day': 4, 'dead': 1, 'deal': 1, 'dear': 3, 'decide': 1, 'decided': 1, 'dedicate': 1, 'dedicated': 1, 'del': 1, 'deleted': 1, 'did': 11, 'didnt': 4, 'dignity': 2, 'dinner': 1, 'dinnermsg': 1, 'directly': 1, 'dirt': 1, 'discuss': 1, 'do': 12, 'does': 5, 'doesnt': 2, 'doing': 5, 'done': 5, 'dont': 10, 'dot': 1, 'double': 2, 'down': 4, 'download': 1, 'dream': 1, 'dresser': 1, 'driving': 1, 'dun': 2, 'e': 3, 'each': 1, 'early': 2, 'earn': 1, 'eat': 1, 'eatin': 1, 'eating': 1, 'egg': 1, 'eggpotato': 1, 'eh': 1, 'eighth': 1, 'ela': 1, 'else': 1, 'embarassed': 1, 'embarassing': 1, 'end': 2, 'endowed': 1, 'enough': 2, 'entered': 1, 'envy': 1, 'escape': 1, 'especially': 1, 'even': 5, 'evening': 1, 'every': 1, 'everyone': 1, 'everywhere': 1, 'excited': 1, 'exist': 1, 'experience': 1, 'factory': 1, 'fainting': 1, 'fair': 1, 'fallen': 1, 'fancy': 1, 'fear': 1, 'feel': 4, 'felt': 1, 'ffffffffff': 1, 'finally': 1, 'find': 1, 'finding': 1, 'fine': 2, 'finish': 6, 'finished': 1, 'finishes': 1, 'first': 6, 'floor': 1, 'flowing': 2, 'food': 1, 'for': 32, 'forced': 1, 'forever': 1, 'forget': 1, 'forgot': 3, 'formal': 1, 'formclark': 1, 'four': 1, 'free': 5, 'friday': 1, 'friend': 1, 'friends': 4, 'frnd': 1, 'frnds': 1, 'from': 2, 'frying': 1, 'fulfil': 1, 'fyi': 1, 'gave': 1, 'gentleman': 2, 'get': 6, 'gets': 1, 'getting': 3, 'girl': 1, 'girls': 1, 'give': 5, 'go': 11, 'goes': 1, 'going': 7, 'gone': 1, 'gonna': 3, 'good': 3, 'goodfine': 1, 'goodo': 1, 'got': 10, 'gota': 1, 'gotta': 1, 'gr': 2, 'gram': 2, 'granted': 1, 'great': 7, 'greatbye': 1, 'grumpy': 1, 'gud': 2, 'guess': 3, 'guys': 1, 'ha': 5, 'had': 1, 'haf': 1, 'haha': 1, 'hail': 1, 'hair': 1, 'hairdressers': 1, 'half': 2, 'hamster': 1, 'hand': 1, 'handsome': 1, 'happened': 1, 'happy': 4, 'hard': 1, 'has': 4, 'hasnt': 1, 'hav': 1, 'have': 22, 'havent': 2, 'he': 11, 'hearts': 1, 'hell': 1, 'hello': 3, 'help': 1, 'hep': 1, 'her': 6, 'here': 5, 'hes': 3, 'hey': 2, 'hi': 8, 'him': 1, 'his': 2, 'hit': 1, 'hmmm': 1, 'hmmmy': 1, 'hols': 1, 'home': 8, 'hop': 1, 'hope': 4, 'hopefully': 1, 'hopes': 1, 'hor': 1, 'hospital': 1, 'hospitals': 1, 'house': 2, 'housework': 1, 'how': 9, 'however': 1, 'hows': 1, 'hungry': 1, 'hurts': 2, 'i': 102, 'if': 11, 'ill': 7, 'im': 25, 'immunisation': 1, 'imposed': 1, 'in': 28, 'inches': 1, 'includes': 1, 'informed': 1, 'inour': 1, 'interview': 1, 'invite': 1, 'invited': 1, 'involve': 1, 'ip': 1, 'is': 19, 'isnt': 1, 'it': 14, 'its': 8, 'itself': 1, 'ive': 3, 'jacket': 1, 'job': 4, 'joined': 1, 'joke': 1, 'jokes': 1, 'joking': 2, 'joy': 1, 'jurong': 1, 'jus': 1, 'just': 11, 'juz': 1, 'k': 6, 'kanoil': 1, 'kate': 1, 'keep': 2, 'kept': 1, 'ki': 1, 'killing': 1, 'kim': 1, 'kkhow': 1, 'kkwhere': 1, 'know': 9, 'knows': 1, 'knowyetunde': 1, 'la': 1, 'lager': 1, 'lar': 3, 'late': 1, 'later': 5, 'lazy': 2, 'league': 1, 'learn': 1, 'leaving': 1, 'lect': 1, 'left': 1, 'legal': 1, 'lesson': 1, 'let': 4, 'letter': 1, 'liao': 1, 'lido': 1, 'life': 3, 'lifted': 1, 'like': 15, 'liked': 1, 'little': 2, 'live': 1, 'lives': 1, 'll': 1, 'loads': 1, 'loans': 1, 'location': 1, 'log': 1, 'lol': 4, 'long': 2, 'look': 2, 'looking': 2, 'lor': 3, 'lot': 2, 'lots': 1, 'loud': 1, 'lovable': 1, 'love': 5, 'loves': 1, 'ltURLgt': 0, 'ltgt': 6, 'lucky': 1, 'lunch': 4, 'lying': 1, 'm': 1, 'machan': 1, 'mah': 2, 'mail': 1, 'make': 5, 'making': 1, 'malarky': 1, 'mall': 1, 'mallika': 1, 'man': 3, 'maneesha': 1, 'mark': 2, 'may': 1, 'maybe': 1, 'me': 29, 'mean': 2, 'means': 1, 'meare': 1, 'meet': 4, 'meeting': 1, 'melle': 6, 'memorable': 1, 'men': 1, 'minecraft': 1, 'minnaminunginte': 3, 'minute': 1, 'miss': 2, 'missing': 1, 'missunderstding': 1, 'mist': 1, 'mmmmmm': 1, 'module': 1, 'mom': 1, 'moms': 1, 'money': 4, 'month': 2, 'months': 1, 'more': 3, 'morning': 1, 'most': 1, 'mouth': 1, 'move': 1, 'moviewat': 1, 'mr': 1, 'mrng': 1, 'mrt': 1, 'msg': 1, 'msn': 1, 'mu': 1, 'much': 4, 'multis': 1, 'mummy': 1, 'must': 2, 'muz': 1, 'my': 39, 'myself': 2, 'n': 4, 'nah': 1, 'name': 4, 'naughty': 1, 'nd': 1, 'need': 7, 'needed': 1, 'needs': 1, 'net': 2, 'network': 1, 'never': 1, 'nevering': 1, 'new': 5, 'next': 1, 'ni': 1, 'nice': 3, 'nigeria': 1, 'night': 2, 'nitros': 1, 'no': 9, 'not': 18, 'nothing': 2, 'noun': 1, 'now': 11, 'nurungu': 3, 'nver': 1, 'nyc': 1, 'occupy': 1, 'odi': 1, 'of': 20, 'offer': 1, 'offered': 1, 'oh': 1, 'ok': 10, 'okay': 1, 'old': 1, 'on': 22, 'once': 2, 'one': 3, 'ones': 1, 'oni': 1, 'only': 7, 'oops': 1, 'open': 2, 'openin': 1, 'operate': 1, 'or': 4, 'orchard': 1, 'ors': 1, 'oru': 3, 'our': 2, 'out': 7, 'over': 2, 'pa': 2, 'packing': 1, 'page': 1, 'pain': 4, 'parentsi': 1, 'part': 2, 'patent': 1, 'pay': 2, 'paying': 1, 'people': 2, 'peoples': 1, 'per': 3, 'performed': 1, 'personal': 1, 'persons': 2, 'pick': 2, 'pizza': 1, 'place': 4, 'plane': 1, 'planning': 1, 'play': 1, 'plaza': 1, 'please': 1, 'pleasure': 1, 'pls': 4, 'plural': 1, 'pm': 1, 'point': 1, 'points': 1, 'pouch': 1, 'pours': 1, 'pray': 1, 'predict': 1, 'press': 3, 'price': 1, 'prob': 1, 'promise': 2, 'puttin': 1, 'qatar': 1, 'question': 1, 'quick': 1, 'r': 3, 'radio': 1, 'rain': 2, 'ratio': 1, 'reached': 2, 'real': 2, 'realized': 1, 'really': 2, 'remember': 2, 'reply': 3, 'request': 3, 'requests': 1, 'research': 1, 'respect': 2, 'ride': 1, 'right': 2, 'room': 3, 'roommates': 2, 'rooms': 1, 'rply': 1, 'run': 1, 'runs': 1, 'safe': 1, 'said': 4, 'same': 1, 'sao': 1, 'sarcastic': 1, 'satisfied': 1, 'saturday': 1, 'save': 1, 'saw': 2, 'say': 3, 'says': 1, 'scared': 1, 'school': 2, 'search': 1, 'searching': 1, 'second': 1, 'see': 9, 'seekers': 1, 'seemed': 1, 'sees': 1, 'sehwag': 1, 'send': 2, 'sending': 1, 'sent': 2, 'sentence': 1, 'series': 1, 'seriously': 1, 'server': 1, 'set': 3, 'settled': 1, 'she': 6, 'sheets': 1, 'sherawat': 1, 'shirt': 1, 'short': 1, 'shouldnt': 1, 'show': 2, 'shower': 1, 'shows': 1, 'shy': 1, 'sick': 1, 'signin': 1, 'since': 1, 'sindu': 1, 'sir': 2, 'sis': 1, 'situation': 1, 'slice': 1, 'smarter': 1, 'smile': 5, 'smoke': 1, 'sms': 1, 'smth': 1, 'so': 20, 'soft': 1, 'some': 1, 'someone': 2, 'something': 3, 'sometimes': 1, 'song': 2, 'soon': 1, 'sooner': 1, 'sorry': 9, 'spanish': 1, 'speak': 2, 'special': 2, 'spell': 2, 'spend': 1, 'spent': 1, 'spoilt': 1, 'spoke': 1, 'staff': 1, 'stand': 1, 'started': 1, 'staying': 1, 'stays': 1, 'steed': 1, 'still': 7, 'stock': 1, 'stool': 1, 'stop': 2, 'story': 1, 'str': 1, 'stubborn': 1, 'studying': 1, 'stuff': 2, 'stuffmoro': 1, 'sucker': 1, 'suckers': 1, 'sucks': 1, 'suggest': 1, 'sum': 2, 'sunday': 1, 'sure': 3, 'surname': 1, 'sweet': 1, 'swt': 1, 'take': 1, 'taking': 1, 'talk': 1, 'tas': 1, 'tea': 1, 'tell': 4, 'telling': 1, 'telugu': 1, 'test': 2, 'text': 4, 'texting': 1, 'thank': 1, 'thanks': 2, 'that': 30, 'thats': 5, 'the': 40, 'their': 1, 'them': 1, 'then': 7, 'there': 7, 'theres': 1, 'they': 1, 'things': 4, 'think': 2, 'thinked': 1, 'this': 5, 'thk': 2, 'tho': 2, 'those': 1, 'though': 2, 'thought': 2, 'tickets': 1, 'til': 1, 'till': 1, 'time': 9, 'times': 3, 'timings': 1, 'tired': 1, 'tmorrowpls': 1, 'tmr': 1, 'to': 53, 'today': 4, 'toll': 1, 'tomo': 2, 'tomorrow': 2, 'tonight': 3, 'too': 7, 'took': 1, 'tortilla': 1, 'touch': 1, 'towards': 1, 'treat': 3, 'trouble': 1, 'truly': 1, 'trust': 1, 'try': 2, 'trying': 1, 'tt': 1, 'turn': 1, 'turns': 1, 'tv': 1, 'txt': 1, 'tyler': 1, 'type': 1, 'typical': 1, 'u': 28, 'umma': 2, 'ummmawill': 1, 'uncle': 1, 'until': 3, 'up': 7, 'ur': 7, 'urgnt': 1, 'us': 2, 'used': 1, 'usf': 1, 'using': 1, 'usually': 1, 'utter': 1, 'v': 6, 'vaguely': 1, 'valuable': 1, 'vava': 1, 'very': 4, 'vettam': 3, 'wa': 1, 'wah': 1, 'wait': 1, 'waiting': 2, 'wanna': 1, 'want': 4, 'wanted': 1, 'was': 6, 'waste': 1, 'wat': 2, 'watching': 2, 'watts': 1, 'way': 7, 'we': 6, 'weak': 2, 'wed': 1, 'week': 1, 'weekend': 2, 'weighthaha': 1, 'well': 3, 'wen': 2, 'went': 1, 'were': 4, 'wet': 1, 'what': 7, 'whats': 1, 'when': 14, 'where': 2, 'wheres': 1, 'which': 2, 'who': 2, 'whole': 2, 'why': 2, 'wif': 3, 'will': 16, 'windows': 1, 'wine': 1, 'wishes': 1, 'wishin': 1, 'wit': 1, 'with': 15, 'without': 1, 'wk': 1, 'wonderful': 1, 'wont': 3, 'words': 1, 'work': 3, 'working': 1, 'world': 2, 'worried': 1, 'worry': 1, 'worth': 1, 'would': 1, 'wow': 2, 'wun': 1, 'x': 3, 'xmas': 1, 'xuhui': 1, 'xx': 2, 'xxx': 1, 'y': 2, 'ya': 2, 'yeah': 3, 'year': 1, 'yes': 7, 'yesgauti': 1, 'yesterday': 1, 'yet': 2, 'yijuehotmailcom': 1, 'yo': 1, 'you': 87, 'youd': 1, 'youhow': 1, 'youll': 1, 'your': 21, 'youre': 2, 'yourself': 3, 'youve': 2, 'yup': 3}\n"
     ]
    }
   ],
   "source": [
    "# Get word frequencies over all statement documents\n",
    "\n",
    "freq_list_ham = word_freq_table_ham.sum(axis=0) \n",
    "freq_ham = dict(zip(word_list_ham,freq_list_ham))\n",
    "print(freq_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        AJ  Expires  LDNWARW  PPM  SPTV  SPTyrone  a  about  ac  ...  \\\n",
      "0   27   0        0        0    0     0         0  1      0   0  ...   \n",
      "1   31   0        0        0    0     0         0  0      0   0  ...   \n",
      "2   25   0        0        0    0     0         0  1      0   0  ...   \n",
      "3   28   0        0        0    0     0         0  0      0   0  ...   \n",
      "4   25   0        0        0    0     0         0  0      0   0  ...   \n",
      "5   25   0        0        0    0     0         0  1      0   0  ...   \n",
      "6   18   0        0        0    0     0         0  0      0   0  ...   \n",
      "7   23   0        0        0    0     0         0  0      0   0  ...   \n",
      "8   28   0        0        0    0     0         0  0      0   0  ...   \n",
      "9   32   0        0        0    0     0         0  1      0   0  ...   \n",
      "10  21   0        0        0    0     0         0  0      0   1  ...   \n",
      "11  26   0        0        0    0     0         0  0      0   0  ...   \n",
      "12  27   0        0        0    0     0         0  2      0   0  ...   \n",
      "13  21   0        0        0    0     0         0  1      0   0  ...   \n",
      "14  14   0        0        0    0     0         0  0      1   0  ...   \n",
      "15  22   0        0        0    0     0         0  1      0   0  ...   \n",
      "16  28   0        0        0    0     0         0  0      0   0  ...   \n",
      "17  26   0        0        0    0     0         0  1      0   0  ...   \n",
      "18  26   0        0        0    0     0         0  3      0   0  ...   \n",
      "19  19   0        0        0    0     0         0  0      0   0  ...   \n",
      "20  25   0        0        0    0     0         0  0      0   0  ...   \n",
      "21  26   0        0        0    0     0         0  2      0   0  ...   \n",
      "22  24   0        0        0    0     0         0  1      0   0  ...   \n",
      "23  27   0        0        0    0     0         0  0      0   0  ...   \n",
      "24  31   0        0        0    0     0         0  0      0   0  ...   \n",
      "25  28   0        0        0    0     0         0  0      0   0  ...   \n",
      "26  18   0        0        0    0     0         0  1      0   0  ...   \n",
      "27  26   0        0        0    0     0         0  3      0   0  ...   \n",
      "28  23   0        0        0    0     0         0  0      0   0  ...   \n",
      "29  29   0        0        0    0     0         0  1      0   0  ...   \n",
      "30  26   0        0        0    0     0         0  1      0   0  ...   \n",
      "31  23   0        0        0    0     0         0  1      0   0  ...   \n",
      "32   9   0        0        0    0     0         0  0      0   0  ...   \n",
      "\n",
      "    xxxmobilemovieclub  xxxmobilemovieclubcomnQJKGIGHJJGCBL  year  years  yes  \\\n",
      "0                    0                                    0     0      0    0   \n",
      "1                    0                                    0     0      0    0   \n",
      "2                    0                                    0     0      0    0   \n",
      "3                    0                                    0     0      0    0   \n",
      "4                    0                                    0     0      0    0   \n",
      "5                    0                                    0     0      0    0   \n",
      "6                    1                                    0     0      0    0   \n",
      "7                    0                                    0     0      0    0   \n",
      "8                    0                                    0     0      0    1   \n",
      "9                    0                                    0     0      0    0   \n",
      "10                   0                                    0     0      0    0   \n",
      "11                   0                                    0     1      0    0   \n",
      "12                   0                                    0     0      0    0   \n",
      "13                   0                                    0     0      0    0   \n",
      "14                   0                                    0     0      0    0   \n",
      "15                   0                                    0     0      0    0   \n",
      "16                   0                                    0     0      0    0   \n",
      "17                   0                                    0     0      0    0   \n",
      "18                   0                                    0     0      0    0   \n",
      "19                   0                                    0     0      0    0   \n",
      "20                   0                                    0     0      0    0   \n",
      "21                   0                                    0     0      0    0   \n",
      "22                   0                                    0     0      0    0   \n",
      "23                   0                                    0     0      0    0   \n",
      "24                   0                                    0     0      0    0   \n",
      "25                   0                                    0     0      0    0   \n",
      "26                   0                                    0     0      1    0   \n",
      "27                   0                                    0     0      0    0   \n",
      "28                   0                                    0     0      0    1   \n",
      "29                   0                                    0     0      0    0   \n",
      "30                   0                                    0     0      0    0   \n",
      "31                   0                                    0     0      0    0   \n",
      "32                   0                                    0     0      0    0   \n",
      "\n",
      "    you  youll  your  yours  yr  \n",
      "0     0      0     0      0   0  \n",
      "1     1      0     0      0   0  \n",
      "2     1      0     0      0   0  \n",
      "3     0      0     1      0   0  \n",
      "4     0      0     0      0   0  \n",
      "5     1      0     0      0   0  \n",
      "6     0      0     1      0   0  \n",
      "7     0      0     0      0   0  \n",
      "8     2      0     2      0   0  \n",
      "9     1      0     1      0   0  \n",
      "10    0      0     0      0   0  \n",
      "11    0      0     0      1   0  \n",
      "12    2      0     1      0   0  \n",
      "13    0      0     0      0   0  \n",
      "14    1      0     0      0   0  \n",
      "15    1      0     0      0   0  \n",
      "16    0      0     1      0   0  \n",
      "17    2      0     0      0   0  \n",
      "18    1      0     0      0   0  \n",
      "19    0      0     1      0   0  \n",
      "20    0      0     1      0   0  \n",
      "21    1      0     0      0   0  \n",
      "22    0      0     0      0   0  \n",
      "23    0      0     0      0   0  \n",
      "24    0      1     0      0   1  \n",
      "25    1      0     0      0   0  \n",
      "26    2      0     0      0   0  \n",
      "27    1      0     0      0   0  \n",
      "28    1      0     0      0   0  \n",
      "29    0      0     0      0   0  \n",
      "30    2      0     0      0   0  \n",
      "31    1      0     0      0   0  \n",
      "32    1      0     0      0   0  \n",
      "\n",
      "[33 rows x 374 columns]\n"
     ]
    }
   ],
   "source": [
    "word_list_spam = get_words(spam_docs)\n",
    "word_freq_table_spam = get_doc_word_frequency(word_list_spam, spam_docs)\n",
    "tdm_spam = pd.DataFrame(word_freq_table_spam, columns=word_list_spam)\n",
    "print(tdm_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 807, 'AJ': 0, 'Expires': 0, 'LDNWARW': 0, 'PPM': 0, 'SPTV': 0, 'SPTyrone': 0, 'a': 22, 'about': 1, 'ac': 1, 'account': 1, 'acoentry': 1, 'advise': 1, 'again': 1, 'age': 1, 'algarve': 1, 'all': 2, 'am': 1, 'ampm': 2, 'and': 5, 'annoncement': 1, 'ansr': 1, 'any': 1, 'app': 1, 'apply': 2, 'are': 7, 'arrange': 1, 'as': 4, 'august': 1, 'award': 1, 'awarded': 3, 'back': 1, 'bangb': 1, 'bangbabes': 1, 'barbie': 1, 'be': 4, 'been': 4, 'between': 2, 'bonus': 3, 'bootydelious': 1, 'box': 1, 'boxwrc': 1, 'britney': 1, 'bt': 1, 'burns': 1, 'bxipwe': 1, 'by': 1, 'c': 1, 'call': 14, 'caller': 1, 'camcorder': 1, 'camera': 1, 'cash': 4, 'chances': 1, 'charged': 2, 'chat': 1, 'chgs': 1, 'cinema': 1, 'claim': 10, 'click': 2, 'co': 1, 'code': 5, 'collected': 1, 'colour': 1, 'comes': 1, 'comp': 1, 'complimentary': 1, 'confirm': 1, 'congrats': 1, 'contact': 3, 'content': 1, 'correct': 1, 'cost': 1, 'country': 1, 'credit': 1, 'csh': 1, 'cup': 1, 'customer': 5, 'darling': 1, 'days': 1, 'delivery': 3, 'detroit': 1, 'devils': 1, 'did': 1, 'direct': 1, 'dis': 1, 'divorce': 1, 'do': 1, 'dogging': 2, 'dont': 2, 'download': 1, 'draw': 2, 'dvd': 1, 'eca': 1, 'eg': 1, 'end': 3, 'ending': 1, 'england': 2, 'enough': 1, 'entitled': 1, 'entry': 2, 'etc': 1, 'eurodisinc': 1, 'f': 1, 'fa': 2, 'female': 1, 'final': 2, 'find': 1, 'flights': 2, 'fml': 1, 'following': 1, 'for': 9, 'free': 9, 'freemsg': 2, 'freephone': 1, 'friend': 1, 'frnd': 1, 'from': 5, 'fun': 1, 'gent': 1, 'get': 3, 'go': 1, 'goalsteam': 1, 'goto': 1, 'gravel': 1, 'guaranteed': 4, 'h': 1, 'had': 1, 'hardcore': 1, 'have': 9, 'havent': 1, 'hear': 2, 'her': 2, 'here': 1, 'hey': 1, 'hl': 1, 'hockey': 1, 'holiday': 2, 'hours': 1, 'hrs': 2, 'httpwap': 1, 'i': 1, 'ice': 1, 'id': 1, 'identifier': 1, 'if': 5, 'im': 1, 'in': 4, 'inc': 2, 'incorrect': 1, 'info': 1, 'internetservice': 1, 'inviting': 1, 'is': 6, 'it': 2, 'its': 1, 'jackpot': 1, 'jersey': 1, 'join': 1, 'k': 1, 'kens': 1, 'kl': 1, 'know': 1, 'laid': 1, 'landline': 1, 'largest': 1, 'last': 2, 'latest': 1, 'lccltd': 1, 'like': 1, 'link': 1, 'live': 3, 'local': 1, 'locations': 1, 'ls': 1, 'ltd': 1, 'luv': 1, 'macedonia': 1, 'match': 1, 'matrix': 1, 'may': 1, 'membership': 1, 'menu': 1, 'message': 1, 'miss': 2, 'mix': 1, 'mk': 1, 'mob': 2, 'mobile': 6, 'mobiles': 1, 'month': 1, 'months': 1, 'more': 2, 'morefrmmob': 1, 'msg': 3, 'msgs': 1, 'must': 1, 'my': 1, 'national': 1, 'netcollex': 1, 'network': 3, 'new': 3, 'news': 1, 'next': 1, 'no': 7, 'nokia': 1, 'not': 3, 'nothing': 1, 'now': 4, 'nt': 1, 'numbers': 1, 'of': 2, 'ok': 1, 'on': 6, 'only': 3, 'operator': 2, 'or': 10, 'order': 1, 'our': 5, 'out': 2, 'overs': 1, 'p': 1, 'pass': 1, 'password': 1, 'pday': 1, 'per': 1, 'play': 1, 'player': 1, 'please': 6, 'pleased': 1, 'pls': 1, 'pm': 1, 'pmin': 1, 'pmsgp': 1, 'po': 1, 'pobox': 1, 'poboxoxwwq': 1, 'points': 1, 'pounds': 1, 'ppm': 2, 'ppw': 1, 'private': 1, 'prize': 8, 'q': 1, 'questionstd': 1, 'quiz': 1, 'quoting': 1, 'r': 1, 'randy': 1, 'rates': 1, 'ratetcs': 1, 'rcv': 2, 're': 1, 'real': 1, 'receive': 5, 'receivea': 1, 'recent': 1, 'red': 1, 'replied': 1, 'reply': 6, 'replying': 1, 'representative': 2, 'review': 1, 'reward': 1, 'ringtone': 2, 'rodger': 1, 's': 1, 'scotland': 1, 'see': 1, 'selected': 4, 'send': 3, 'sent': 1, 'service': 4, 'services': 1, 'sexy': 1, 'should': 1, 'shows': 3, 'shracomorsglsuplt': 1, 'simply': 1, 'six': 1, 'sms': 2, 'some': 1, 'sony': 1, 'speak': 2, 'special': 1, 'specially': 2, 'sptv': 2, 'st': 1, 'standard': 1, 'starwars': 1, 'statement': 1, 'std': 1, 'still': 1, 'stop': 4, 'stuff': 1, 'subscription': 1, 'sunshine': 1, 'suprman': 1, 'svc': 1, 'tb': 1, 'tc': 1, 'team': 1, 'text': 4, 'th': 1, 'thanks': 1, 'that': 3, 'the': 14, 'there': 1, 'this': 1, 'tkts': 1, 'to': 42, 'todays': 1, 'tomorrow': 1, 'tonight': 1, 'top': 1, 'trav': 1, 'tried': 1, 'trip': 1, 'try': 2, 'trying': 2, 'trywales': 1, 'tsandcs': 1, 'tv': 1, 'txt': 8, 'txting': 1, 'u': 10, 'uk': 1, 'uks': 1, 'unique': 1, 'unredeemed': 1, 'up': 1, 'update': 2, 'ur': 6, 'urgent': 4, 'use': 1, 'usher': 1, 'v': 2, 'valid': 3, 'valued': 2, 'verify': 2, 'voda': 1, 'waiting': 2, 'want': 2, 'wap': 2, 'was': 1, 'way': 1, 'we': 3, 'week': 1, 'weekends': 2, 'weeks': 1, 'which': 1, 'why': 1, 'will': 2, 'win': 3, 'wings': 1, 'winner': 3, 'with': 4, 'wkly': 2, 'won': 5, 'word': 2, 'wwwareyouuniquecouk': 1, 'wwwdbuknet': 1, 'wwwsmsacubootydelious': 1, 'xxx': 1, 'xxxmobilemovieclub': 1, 'xxxmobilemovieclubcomnQJKGIGHJJGCBL': 0, 'year': 1, 'years': 1, 'yes': 2, 'you': 23, 'youll': 1, 'your': 9, 'yours': 1, 'yr': 1}\n",
      "[2376    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   40    1    1    5    2    1    1    1    1    1    3    1    4\n",
      "    2    3    1    2    1    1    1   16    1    8    1    1    3   11\n",
      "    1    1    1    2   37    1    1    1    1    2    1    7    1    1\n",
      "    1    1    1    1    1    1    1    2   19    2    9    1    1   13\n",
      "    1    1    1    1    1    3    2    1    7    1    1    2    1    1\n",
      "    1    9    1    1    1    1    7    1    1    1    1    3    1    1\n",
      "    2    2    1    1    2    3    1    2    1    2    1    1    1    1\n",
      "    1    1    1    1    1    1    1   13    3    1    2    3    1   12\n",
      "    3    6    1    1    1   11    3    4    1    2    1    2    1    1\n",
      "    1    2    4    1    1    1    1    2    3    1    2    1    1    1\n",
      "    2    1    1    1    1    1    1    1    1    1    1    3    3    2\n",
      "    2    1    1    1    1    1    1    1    1    1    2    1    1    2\n",
      "    1    4    1    1    3    1    1    1    1    1    1   11    4    2\n",
      "    1    1    1    1    1   12    5    2    5    5   10    1    2    4\n",
      "    1    1    1    1    2    3    1    2    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    2    1    2    1    1    1    1    5\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    4\n",
      "    1    1    1    1    1    2    6    1    1    6    1    2    1   32\n",
      "    1    1    1    3    1    1    1    5    1    1    4    1    1    2\n",
      "    1    1    1    1    2    6    1    3    1    1    5   11    1    7\n",
      "    1    3    3    1    1   10    1    1    2    2    1    7    1    1\n",
      "    2    3    1    5    1    1    1    1    1    1    2    1    1    1\n",
      "    1    4    1    4    1    1   22    2   11    1    1    3    1    1\n",
      "    6    5    3    2    8    1    2    1    1    1    1    8    1    4\n",
      "    1    1    1    1    1    2    1    9    1    1    1    2  102   11\n",
      "    7   25    1    1   28    1    1    1    1    1    1    1    1    1\n",
      "   19    1   14    8    1    3    1    4    1    1    1    2    1    1\n",
      "    1   11    1    6    1    1    2    1    1    1    1    1    1    9\n",
      "    1    1    1    1    3    1    5    2    1    1    1    1    1    1\n",
      "    1    4    1    1    1    3    1   15    1    2    1    1    1    1\n",
      "    1    1    1    4    2    2    2    3    2    1    1    1    5    1\n",
      "    0    6    1    4    1    1    1    2    1    5    1    1    1    1\n",
      "    3    1    2    1    1   29    2    1    1    4    1    6    1    1\n",
      "    1    3    1    2    1    1    1    1    1    1    1    4    2    1\n",
      "    3    1    1    1    1    1    1    1    1    1    1    1    4    1\n",
      "    1    2    1   39    2    4    1    4    1    1    7    1    1    2\n",
      "    1    1    1    5    1    1    3    1    2    1    9   18    2    1\n",
      "   11    3    1    1    1    1   20    1    1    1   10    1    1   22\n",
      "    2    3    1    1    7    1    2    1    1    4    1    1    3    2\n",
      "    7    2    2    1    1    4    1    2    1    2    1    2    1    3\n",
      "    1    1    2    2    1    4    1    1    1    1    1    1    4    1\n",
      "    1    1    1    1    1    1    1    3    1    1    2    1    1    1\n",
      "    1    3    1    2    1    2    2    1    2    2    3    3    1    1\n",
      "    2    1    2    3    2    1    1    1    1    1    4    1    1    1\n",
      "    1    1    1    2    3    1    1    2    1    1    1    9    1    1\n",
      "    1    1    2    1    2    1    1    1    1    3    1    6    1    1\n",
      "    1    1    1    2    1    1    1    1    1    1    1    2    1    1\n",
      "    1    1    5    1    1    1   20    1    1    2    3    1    2    1\n",
      "    1    9    1    2    2    2    1    1    1    1    1    1    1    1\n",
      "    1    1    7    1    1    2    1    1    1    1    2    1    1    1\n",
      "    1    1    2    1    3    1    1    1    1    1    1    1    1    4\n",
      "    1    1    2    4    1    1    2   30    5   40    1    1    7    7\n",
      "    1    1    4    2    1    5    2    2    1    2    2    1    1    1\n",
      "    9    3    1    1    1    1   53    4    1    2    2    3    7    1\n",
      "    1    1    1    3    1    1    1    2    1    1    1    1    1    1\n",
      "    1    1    1   28    2    1    1    3    7    7    1    2    1    1\n",
      "    1    1    1    6    1    1    1    4    3    1    1    1    2    1\n",
      "    4    1    6    1    2    2    1    7    6    2    1    1    2    1\n",
      "    3    2    1    4    1    7    1   14    2    1    2    2    2    2\n",
      "    3   16    1    1    1    1    1   15    1    1    1    3    1    3\n",
      "    1    2    1    1    1    1    2    1    3    1    1    2    1    2\n",
      "    2    3    1    7    1    1    2    1    1   87    1    1    1   21\n",
      "    2    3    2    3]\n",
      "[807   0   0   0   0   0   0  22   1   1   1   1   1   1   1   1   2   1\n",
      "   2   5   1   1   1   1   2   7   1   4   1   1   3   1   1   1   1   4\n",
      "   4   2   3   1   1   1   1   1   1   1   1   1  14   1   1   1   4   1\n",
      "   2   1   1   1  10   2   1   5   1   1   1   1   1   1   1   3   1   1\n",
      "   1   1   1   1   1   5   1   1   3   1   1   1   1   1   1   1   2   2\n",
      "   1   2   1   1   1   3   1   2   1   1   2   1   1   1   2   1   2   1\n",
      "   2   1   1   9   9   2   1   1   1   5   1   1   3   1   1   1   1   4\n",
      "   1   1   1   9   1   2   2   1   1   1   1   2   1   2   1   1   1   1\n",
      "   1   5   1   4   2   1   1   1   1   6   2   1   1   1   1   1   1   1\n",
      "   1   1   1   1   2   1   1   1   1   3   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   2   1   1   2   6   1   1   1   2   1   3   1   1   1\n",
      "   1   1   3   3   1   1   7   1   3   1   4   1   1   2   1   6   3   2\n",
      "  10   1   5   2   1   1   1   1   1   1   1   1   6   1   1   1   1   1\n",
      "   1   1   1   1   1   2   1   1   8   1   1   1   1   1   1   1   1   2\n",
      "   1   1   5   1   1   1   1   6   1   2   1   1   2   1   1   1   1   4\n",
      "   3   1   4   1   1   1   3   1   1   1   2   1   1   2   1   2   2   1\n",
      "   1   1   1   1   1   4   1   1   1   1   1   1   1   1   4   1   1   3\n",
      "  14   1   1   1  42   1   1   1   1   1   1   1   2   2   1   1   1   8\n",
      "   1  10   1   1   1   1   1   2   6   4   1   1   2   3   2   2   1   2\n",
      "   2   2   1   1   3   1   2   1   1   1   2   3   1   3   4   2   5   2\n",
      "   1   1   1   1   1   0   1   1   2  23   1   9   1   1]\n"
     ]
    }
   ],
   "source": [
    "freq_list_spam = word_freq_table_spam.sum(axis=0) \n",
    "freq_spam = dict(zip(word_list_spam,freq_list_spam))\n",
    "print(freq_spam)\n",
    "print(freq_list_ham)\n",
    "print(freq_list_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of words for \"statement\" class \n",
      "\n",
      "{'': 0.4142558382711746, 'Abiola': 0.0001742767514813524, 'Callertune': 0.0001742767514813524, 'Hee': 0.0001742767514813524, 'IQ': 0.0001742767514813524, 'LUCYxx': 0.0001742767514813524, 'Lol': 0.0001742767514813524, 'No': 0.0001742767514813524, 'SEEING': 0.0001742767514813524, 'Smiling': 0.0001742767514813524, 'Sorry': 0.0001742767514813524, 'WILL': 0.0001742767514813524, 'XX': 0.0001742767514813524, 'You': 0.0001742767514813524, 'Yummy': 0.0001742767514813524, 'a': 0.007145346810735448, 'aaooooright': 0.0003485535029627048, 'able': 0.0003485535029627048, 'about': 0.0010456605088881143, 'abt': 0.0005228302544440571, 'accomodate': 0.0003485535029627048, 'accomodations': 0.0003485535029627048, 'account': 0.0003485535029627048, 'actin': 0.0003485535029627048, 'activities': 0.0003485535029627048, 'address': 0.0006971070059254096, 'aft': 0.0003485535029627048, 'after': 0.000871383757406762, 'afternoon': 0.0005228302544440571, 'again': 0.0006971070059254096, 'ah': 0.0003485535029627048, 'ahead': 0.0005228302544440571, 'ahhh': 0.0003485535029627048, 'aids': 0.0003485535029627048, 'aight': 0.0003485535029627048, 'all': 0.0029627047751829907, 'almost': 0.0003485535029627048, 'already': 0.0015684907633321714, 'alright': 0.0003485535029627048, 'also': 0.0003485535029627048, 'always': 0.0006971070059254096, 'am': 0.0020913210177762286, 'amore': 0.0003485535029627048, 'amp': 0.0003485535029627048, 'ams': 0.0003485535029627048, 'an': 0.0005228302544440571, 'and': 0.006622516556291391, 'animation': 0.0003485535029627048, 'another': 0.0003485535029627048, 'answer': 0.0003485535029627048, 'any': 0.0003485535029627048, 'anymore': 0.0005228302544440571, 'anythin': 0.0003485535029627048, 'anything': 0.0013942140118508191, 'anyway': 0.0003485535029627048, 'anyways': 0.0003485535029627048, 'apartment': 0.0003485535029627048, 'apologetic': 0.0003485535029627048, 'apologise': 0.0003485535029627048, 'applespairsall': 0.0003485535029627048, 'appointment': 0.0003485535029627048, 'approaches': 0.0003485535029627048, 'arabian': 0.0003485535029627048, 'ard': 0.0005228302544440571, 'are': 0.003485535029627048, 'around': 0.0005228302544440571, 'as': 0.001742767514813524, 'ask': 0.0003485535029627048, 'askd': 0.0003485535029627048, 'at': 0.0024398745207389336, 'available': 0.0003485535029627048, 'ave': 0.0003485535029627048, 'avoid': 0.0003485535029627048, 'awesome': 0.0003485535029627048, 'axis': 0.0003485535029627048, 'b': 0.0006971070059254096, 'babe': 0.0005228302544440571, 'babyjontet': 0.0003485535029627048, 'back': 0.0013942140118508191, 'badly': 0.0003485535029627048, 'bak': 0.0003485535029627048, 'bank': 0.0005228302544440571, 'bath': 0.0003485535029627048, 'bathe': 0.0003485535029627048, 'bday': 0.0003485535029627048, 'be': 0.001742767514813524, 'because': 0.0003485535029627048, 'become': 0.0003485535029627048, 'becoz': 0.0003485535029627048, 'bed': 0.0003485535029627048, 'been': 0.0013942140118508191, 'befor': 0.0003485535029627048, 'beforehand': 0.0003485535029627048, 'begin': 0.0003485535029627048, 'begins': 0.0003485535029627048, 'being': 0.0006971070059254096, 'believe': 0.0003485535029627048, 'beloved': 0.0003485535029627048, 'best': 0.0005228302544440571, 'better': 0.0005228302544440571, 'biggest': 0.0003485535029627048, 'birla': 0.0003485535029627048, 'birthday': 0.0005228302544440571, 'bit': 0.0006971070059254096, 'blessing': 0.0003485535029627048, 'bloody': 0.0005228302544440571, 'boss': 0.0003485535029627048, 'boston': 0.0005228302544440571, 'bother': 0.0003485535029627048, 'boy': 0.0003485535029627048, 'boytoy': 0.0003485535029627048, 'breather': 0.0003485535029627048, 'bridge': 0.0003485535029627048, 'brother': 0.0003485535029627048, 'buffet': 0.0003485535029627048, 'bugis': 0.0003485535029627048, 'burger': 0.0003485535029627048, 'bus': 0.0003485535029627048, 'busy': 0.0003485535029627048, 'but': 0.0024398745207389336, 'buy': 0.0006971070059254096, 'buying': 0.0003485535029627048, 'by': 0.0005228302544440571, 'c': 0.0006971070059254096, 'cabin': 0.0003485535029627048, 'call': 0.002265597769257581, 'callers': 0.0006971070059254096, 'callertune': 0.0012199372603694668, 'calls': 0.0003485535029627048, 'callsmessagesmissed': 0.0003485535029627048, 'came': 0.0003485535029627048, 'can': 0.0020913210177762286, 'cant': 0.0006971070059254096, 'car': 0.000871383757406762, 'carlos': 0.0003485535029627048, 'cash': 0.0005228302544440571, 'casualty': 0.0003485535029627048, 'catch': 0.0005228302544440571, 'caught': 0.0003485535029627048, 'cause': 0.0003485535029627048, 'cave': 0.0003485535029627048, 'changed': 0.0005228302544440571, 'check': 0.000871383757406762, 'checking': 0.0003485535029627048, 'cheers': 0.0003485535029627048, 'child': 0.0003485535029627048, 'chores': 0.0003485535029627048, 'cine': 0.0005228302544440571, 'class': 0.0006971070059254096, 'clear': 0.0003485535029627048, 'close': 0.0005228302544440571, 'clue': 0.0003485535029627048, 'coffee': 0.0003485535029627048, 'coins': 0.0003485535029627048, 'come': 0.0005228302544440571, 'comes': 0.0003485535029627048, 'comin': 0.0003485535029627048, 'coming': 0.0003485535029627048, 'completed': 0.0003485535029627048, 'completely': 0.0003485535029627048, 'computer': 0.0003485535029627048, 'considering': 0.0003485535029627048, 'contact': 0.0003485535029627048, 'convincing': 0.0003485535029627048, 'cool': 0.0003485535029627048, 'copy': 0.0006971070059254096, 'cos': 0.0006971070059254096, 'cost': 0.0005228302544440571, 'could': 0.0005228302544440571, 'course': 0.0003485535029627048, 'cozsomtimes': 0.0003485535029627048, 'crashing': 0.0003485535029627048, 'crave': 0.0003485535029627048, 'crazy': 0.0003485535029627048, 'cried': 0.0003485535029627048, 'cultures': 0.0003485535029627048, 'cup': 0.0003485535029627048, 'cuppa': 0.0003485535029627048, 'cut': 0.0005228302544440571, 'cuz': 0.0003485535029627048, 'd': 0.0003485535029627048, 'da': 0.0005228302544440571, 'date': 0.0003485535029627048, 'day': 0.000871383757406762, 'dead': 0.0003485535029627048, 'deal': 0.0003485535029627048, 'dear': 0.0006971070059254096, 'decide': 0.0003485535029627048, 'decided': 0.0003485535029627048, 'dedicate': 0.0003485535029627048, 'dedicated': 0.0003485535029627048, 'del': 0.0003485535029627048, 'deleted': 0.0003485535029627048, 'did': 0.0020913210177762286, 'didnt': 0.000871383757406762, 'dignity': 0.0005228302544440571, 'dinner': 0.0003485535029627048, 'dinnermsg': 0.0003485535029627048, 'directly': 0.0003485535029627048, 'dirt': 0.0003485535029627048, 'discuss': 0.0003485535029627048, 'do': 0.002265597769257581, 'does': 0.0010456605088881143, 'doesnt': 0.0005228302544440571, 'doing': 0.0010456605088881143, 'done': 0.0010456605088881143, 'dont': 0.0019170442662948763, 'dot': 0.0003485535029627048, 'double': 0.0005228302544440571, 'down': 0.000871383757406762, 'download': 0.0003485535029627048, 'dream': 0.0003485535029627048, 'dresser': 0.0003485535029627048, 'driving': 0.0003485535029627048, 'dun': 0.0005228302544440571, 'e': 0.0006971070059254096, 'each': 0.0003485535029627048, 'early': 0.0005228302544440571, 'earn': 0.0003485535029627048, 'eat': 0.0003485535029627048, 'eatin': 0.0003485535029627048, 'eating': 0.0003485535029627048, 'egg': 0.0003485535029627048, 'eggpotato': 0.0003485535029627048, 'eh': 0.0003485535029627048, 'eighth': 0.0003485535029627048, 'ela': 0.0003485535029627048, 'else': 0.0003485535029627048, 'embarassed': 0.0003485535029627048, 'embarassing': 0.0003485535029627048, 'end': 0.0005228302544440571, 'endowed': 0.0003485535029627048, 'enough': 0.0005228302544440571, 'entered': 0.0003485535029627048, 'envy': 0.0003485535029627048, 'escape': 0.0003485535029627048, 'especially': 0.0003485535029627048, 'even': 0.0010456605088881143, 'evening': 0.0003485535029627048, 'every': 0.0003485535029627048, 'everyone': 0.0003485535029627048, 'everywhere': 0.0003485535029627048, 'excited': 0.0003485535029627048, 'exist': 0.0003485535029627048, 'experience': 0.0003485535029627048, 'factory': 0.0003485535029627048, 'fainting': 0.0003485535029627048, 'fair': 0.0003485535029627048, 'fallen': 0.0003485535029627048, 'fancy': 0.0003485535029627048, 'fear': 0.0003485535029627048, 'feel': 0.000871383757406762, 'felt': 0.0003485535029627048, 'ffffffffff': 0.0003485535029627048, 'finally': 0.0003485535029627048, 'find': 0.0003485535029627048, 'finding': 0.0003485535029627048, 'fine': 0.0005228302544440571, 'finish': 0.0012199372603694668, 'finished': 0.0003485535029627048, 'finishes': 0.0003485535029627048, 'first': 0.0012199372603694668, 'floor': 0.0003485535029627048, 'flowing': 0.0005228302544440571, 'food': 0.0003485535029627048, 'for': 0.0057511327988846285, 'forced': 0.0003485535029627048, 'forever': 0.0003485535029627048, 'forget': 0.0003485535029627048, 'forgot': 0.0006971070059254096, 'formal': 0.0003485535029627048, 'formclark': 0.0003485535029627048, 'four': 0.0003485535029627048, 'free': 0.0010456605088881143, 'friday': 0.0003485535029627048, 'friend': 0.0003485535029627048, 'friends': 0.000871383757406762, 'frnd': 0.0003485535029627048, 'frnds': 0.0003485535029627048, 'from': 0.0005228302544440571, 'frying': 0.0003485535029627048, 'fulfil': 0.0003485535029627048, 'fyi': 0.0003485535029627048, 'gave': 0.0003485535029627048, 'gentleman': 0.0005228302544440571, 'get': 0.0012199372603694668, 'gets': 0.0003485535029627048, 'getting': 0.0006971070059254096, 'girl': 0.0003485535029627048, 'girls': 0.0003485535029627048, 'give': 0.0010456605088881143, 'go': 0.0020913210177762286, 'goes': 0.0003485535029627048, 'going': 0.0013942140118508191, 'gone': 0.0003485535029627048, 'gonna': 0.0006971070059254096, 'good': 0.0006971070059254096, 'goodfine': 0.0003485535029627048, 'goodo': 0.0003485535029627048, 'got': 0.0019170442662948763, 'gota': 0.0003485535029627048, 'gotta': 0.0003485535029627048, 'gr': 0.0005228302544440571, 'gram': 0.0005228302544440571, 'granted': 0.0003485535029627048, 'great': 0.0013942140118508191, 'greatbye': 0.0003485535029627048, 'grumpy': 0.0003485535029627048, 'gud': 0.0005228302544440571, 'guess': 0.0006971070059254096, 'guys': 0.0003485535029627048, 'ha': 0.0010456605088881143, 'had': 0.0003485535029627048, 'haf': 0.0003485535029627048, 'haha': 0.0003485535029627048, 'hail': 0.0003485535029627048, 'hair': 0.0003485535029627048, 'hairdressers': 0.0003485535029627048, 'half': 0.0005228302544440571, 'hamster': 0.0003485535029627048, 'hand': 0.0003485535029627048, 'handsome': 0.0003485535029627048, 'happened': 0.0003485535029627048, 'happy': 0.000871383757406762, 'hard': 0.0003485535029627048, 'has': 0.000871383757406762, 'hasnt': 0.0003485535029627048, 'hav': 0.0003485535029627048, 'have': 0.004008365284071105, 'havent': 0.0005228302544440571, 'he': 0.0020913210177762286, 'hearts': 0.0003485535029627048, 'hell': 0.0003485535029627048, 'hello': 0.0006971070059254096, 'help': 0.0003485535029627048, 'hep': 0.0003485535029627048, 'her': 0.0012199372603694668, 'here': 0.0010456605088881143, 'hes': 0.0006971070059254096, 'hey': 0.0005228302544440571, 'hi': 0.0015684907633321714, 'him': 0.0003485535029627048, 'his': 0.0005228302544440571, 'hit': 0.0003485535029627048, 'hmmm': 0.0003485535029627048, 'hmmmy': 0.0003485535029627048, 'hols': 0.0003485535029627048, 'home': 0.0015684907633321714, 'hop': 0.0003485535029627048, 'hope': 0.000871383757406762, 'hopefully': 0.0003485535029627048, 'hopes': 0.0003485535029627048, 'hor': 0.0003485535029627048, 'hospital': 0.0003485535029627048, 'hospitals': 0.0003485535029627048, 'house': 0.0005228302544440571, 'housework': 0.0003485535029627048, 'how': 0.001742767514813524, 'however': 0.0003485535029627048, 'hows': 0.0003485535029627048, 'hungry': 0.0003485535029627048, 'hurts': 0.0005228302544440571, 'i': 0.017950505402579295, 'if': 0.0020913210177762286, 'ill': 0.0013942140118508191, 'im': 0.004531195538515162, 'immunisation': 0.0003485535029627048, 'imposed': 0.0003485535029627048, 'in': 0.005054025792959219, 'inches': 0.0003485535029627048, 'includes': 0.0003485535029627048, 'informed': 0.0003485535029627048, 'inour': 0.0003485535029627048, 'interview': 0.0003485535029627048, 'invite': 0.0003485535029627048, 'invited': 0.0003485535029627048, 'involve': 0.0003485535029627048, 'ip': 0.0003485535029627048, 'is': 0.003485535029627048, 'isnt': 0.0003485535029627048, 'it': 0.0026141512722202857, 'its': 0.0015684907633321714, 'itself': 0.0003485535029627048, 'ive': 0.0006971070059254096, 'jacket': 0.0003485535029627048, 'job': 0.000871383757406762, 'joined': 0.0003485535029627048, 'joke': 0.0003485535029627048, 'jokes': 0.0003485535029627048, 'joking': 0.0005228302544440571, 'joy': 0.0003485535029627048, 'jurong': 0.0003485535029627048, 'jus': 0.0003485535029627048, 'just': 0.0020913210177762286, 'juz': 0.0003485535029627048, 'k': 0.0012199372603694668, 'kanoil': 0.0003485535029627048, 'kate': 0.0003485535029627048, 'keep': 0.0005228302544440571, 'kept': 0.0003485535029627048, 'ki': 0.0003485535029627048, 'killing': 0.0003485535029627048, 'kim': 0.0003485535029627048, 'kkhow': 0.0003485535029627048, 'kkwhere': 0.0003485535029627048, 'know': 0.001742767514813524, 'knows': 0.0003485535029627048, 'knowyetunde': 0.0003485535029627048, 'la': 0.0003485535029627048, 'lager': 0.0003485535029627048, 'lar': 0.0006971070059254096, 'late': 0.0003485535029627048, 'later': 0.0010456605088881143, 'lazy': 0.0005228302544440571, 'league': 0.0003485535029627048, 'learn': 0.0003485535029627048, 'leaving': 0.0003485535029627048, 'lect': 0.0003485535029627048, 'left': 0.0003485535029627048, 'legal': 0.0003485535029627048, 'lesson': 0.0003485535029627048, 'let': 0.000871383757406762, 'letter': 0.0003485535029627048, 'liao': 0.0003485535029627048, 'lido': 0.0003485535029627048, 'life': 0.0006971070059254096, 'lifted': 0.0003485535029627048, 'like': 0.0027884280237016382, 'liked': 0.0003485535029627048, 'little': 0.0005228302544440571, 'live': 0.0003485535029627048, 'lives': 0.0003485535029627048, 'll': 0.0003485535029627048, 'loads': 0.0003485535029627048, 'loans': 0.0003485535029627048, 'location': 0.0003485535029627048, 'log': 0.0003485535029627048, 'lol': 0.000871383757406762, 'long': 0.0005228302544440571, 'look': 0.0005228302544440571, 'looking': 0.0005228302544440571, 'lor': 0.0006971070059254096, 'lot': 0.0005228302544440571, 'lots': 0.0003485535029627048, 'loud': 0.0003485535029627048, 'lovable': 0.0003485535029627048, 'love': 0.0010456605088881143, 'loves': 0.0003485535029627048, 'ltURLgt': 0.0001742767514813524, 'ltgt': 0.0012199372603694668, 'lucky': 0.0003485535029627048, 'lunch': 0.000871383757406762, 'lying': 0.0003485535029627048, 'm': 0.0003485535029627048, 'machan': 0.0003485535029627048, 'mah': 0.0005228302544440571, 'mail': 0.0003485535029627048, 'make': 0.0010456605088881143, 'making': 0.0003485535029627048, 'malarky': 0.0003485535029627048, 'mall': 0.0003485535029627048, 'mallika': 0.0003485535029627048, 'man': 0.0006971070059254096, 'maneesha': 0.0003485535029627048, 'mark': 0.0005228302544440571, 'may': 0.0003485535029627048, 'maybe': 0.0003485535029627048, 'me': 0.005228302544440571, 'mean': 0.0005228302544440571, 'means': 0.0003485535029627048, 'meare': 0.0003485535029627048, 'meet': 0.000871383757406762, 'meeting': 0.0003485535029627048, 'melle': 0.0012199372603694668, 'memorable': 0.0003485535029627048, 'men': 0.0003485535029627048, 'minecraft': 0.0003485535029627048, 'minnaminunginte': 0.0006971070059254096, 'minute': 0.0003485535029627048, 'miss': 0.0005228302544440571, 'missing': 0.0003485535029627048, 'missunderstding': 0.0003485535029627048, 'mist': 0.0003485535029627048, 'mmmmmm': 0.0003485535029627048, 'module': 0.0003485535029627048, 'mom': 0.0003485535029627048, 'moms': 0.0003485535029627048, 'money': 0.000871383757406762, 'month': 0.0005228302544440571, 'months': 0.0003485535029627048, 'more': 0.0006971070059254096, 'morning': 0.0003485535029627048, 'most': 0.0003485535029627048, 'mouth': 0.0003485535029627048, 'move': 0.0003485535029627048, 'moviewat': 0.0003485535029627048, 'mr': 0.0003485535029627048, 'mrng': 0.0003485535029627048, 'mrt': 0.0003485535029627048, 'msg': 0.0003485535029627048, 'msn': 0.0003485535029627048, 'mu': 0.0003485535029627048, 'much': 0.000871383757406762, 'multis': 0.0003485535029627048, 'mummy': 0.0003485535029627048, 'must': 0.0005228302544440571, 'muz': 0.0003485535029627048, 'my': 0.006971070059254096, 'myself': 0.0005228302544440571, 'n': 0.000871383757406762, 'nah': 0.0003485535029627048, 'name': 0.000871383757406762, 'naughty': 0.0003485535029627048, 'nd': 0.0003485535029627048, 'need': 0.0013942140118508191, 'needed': 0.0003485535029627048, 'needs': 0.0003485535029627048, 'net': 0.0005228302544440571, 'network': 0.0003485535029627048, 'never': 0.0003485535029627048, 'nevering': 0.0003485535029627048, 'new': 0.0010456605088881143, 'next': 0.0003485535029627048, 'ni': 0.0003485535029627048, 'nice': 0.0006971070059254096, 'nigeria': 0.0003485535029627048, 'night': 0.0005228302544440571, 'nitros': 0.0003485535029627048, 'no': 0.001742767514813524, 'not': 0.0033112582781456954, 'nothing': 0.0005228302544440571, 'noun': 0.0003485535029627048, 'now': 0.0020913210177762286, 'nurungu': 0.0006971070059254096, 'nver': 0.0003485535029627048, 'nyc': 0.0003485535029627048, 'occupy': 0.0003485535029627048, 'odi': 0.0003485535029627048, 'of': 0.0036598117811084, 'offer': 0.0003485535029627048, 'offered': 0.0003485535029627048, 'oh': 0.0003485535029627048, 'ok': 0.0019170442662948763, 'okay': 0.0003485535029627048, 'old': 0.0003485535029627048, 'on': 0.004008365284071105, 'once': 0.0005228302544440571, 'one': 0.0006971070059254096, 'ones': 0.0003485535029627048, 'oni': 0.0003485535029627048, 'only': 0.0013942140118508191, 'oops': 0.0003485535029627048, 'open': 0.0005228302544440571, 'openin': 0.0003485535029627048, 'operate': 0.0003485535029627048, 'or': 0.000871383757406762, 'orchard': 0.0003485535029627048, 'ors': 0.0003485535029627048, 'oru': 0.0006971070059254096, 'our': 0.0005228302544440571, 'out': 0.0013942140118508191, 'over': 0.0005228302544440571, 'pa': 0.0005228302544440571, 'packing': 0.0003485535029627048, 'page': 0.0003485535029627048, 'pain': 0.000871383757406762, 'parentsi': 0.0003485535029627048, 'part': 0.0005228302544440571, 'patent': 0.0003485535029627048, 'pay': 0.0005228302544440571, 'paying': 0.0003485535029627048, 'people': 0.0005228302544440571, 'peoples': 0.0003485535029627048, 'per': 0.0006971070059254096, 'performed': 0.0003485535029627048, 'personal': 0.0003485535029627048, 'persons': 0.0005228302544440571, 'pick': 0.0005228302544440571, 'pizza': 0.0003485535029627048, 'place': 0.000871383757406762, 'plane': 0.0003485535029627048, 'planning': 0.0003485535029627048, 'play': 0.0003485535029627048, 'plaza': 0.0003485535029627048, 'please': 0.0003485535029627048, 'pleasure': 0.0003485535029627048, 'pls': 0.000871383757406762, 'plural': 0.0003485535029627048, 'pm': 0.0003485535029627048, 'point': 0.0003485535029627048, 'points': 0.0003485535029627048, 'pouch': 0.0003485535029627048, 'pours': 0.0003485535029627048, 'pray': 0.0003485535029627048, 'predict': 0.0003485535029627048, 'press': 0.0006971070059254096, 'price': 0.0003485535029627048, 'prob': 0.0003485535029627048, 'promise': 0.0005228302544440571, 'puttin': 0.0003485535029627048, 'qatar': 0.0003485535029627048, 'question': 0.0003485535029627048, 'quick': 0.0003485535029627048, 'r': 0.0006971070059254096, 'radio': 0.0003485535029627048, 'rain': 0.0005228302544440571, 'ratio': 0.0003485535029627048, 'reached': 0.0005228302544440571, 'real': 0.0005228302544440571, 'realized': 0.0003485535029627048, 'really': 0.0005228302544440571, 'remember': 0.0005228302544440571, 'reply': 0.0006971070059254096, 'request': 0.0006971070059254096, 'requests': 0.0003485535029627048, 'research': 0.0003485535029627048, 'respect': 0.0005228302544440571, 'ride': 0.0003485535029627048, 'right': 0.0005228302544440571, 'room': 0.0006971070059254096, 'roommates': 0.0005228302544440571, 'rooms': 0.0003485535029627048, 'rply': 0.0003485535029627048, 'run': 0.0003485535029627048, 'runs': 0.0003485535029627048, 'safe': 0.0003485535029627048, 'said': 0.000871383757406762, 'same': 0.0003485535029627048, 'sao': 0.0003485535029627048, 'sarcastic': 0.0003485535029627048, 'satisfied': 0.0003485535029627048, 'saturday': 0.0003485535029627048, 'save': 0.0003485535029627048, 'saw': 0.0005228302544440571, 'say': 0.0006971070059254096, 'says': 0.0003485535029627048, 'scared': 0.0003485535029627048, 'school': 0.0005228302544440571, 'search': 0.0003485535029627048, 'searching': 0.0003485535029627048, 'second': 0.0003485535029627048, 'see': 0.001742767514813524, 'seekers': 0.0003485535029627048, 'seemed': 0.0003485535029627048, 'sees': 0.0003485535029627048, 'sehwag': 0.0003485535029627048, 'send': 0.0005228302544440571, 'sending': 0.0003485535029627048, 'sent': 0.0005228302544440571, 'sentence': 0.0003485535029627048, 'series': 0.0003485535029627048, 'seriously': 0.0003485535029627048, 'server': 0.0003485535029627048, 'set': 0.0006971070059254096, 'settled': 0.0003485535029627048, 'she': 0.0012199372603694668, 'sheets': 0.0003485535029627048, 'sherawat': 0.0003485535029627048, 'shirt': 0.0003485535029627048, 'short': 0.0003485535029627048, 'shouldnt': 0.0003485535029627048, 'show': 0.0005228302544440571, 'shower': 0.0003485535029627048, 'shows': 0.0003485535029627048, 'shy': 0.0003485535029627048, 'sick': 0.0003485535029627048, 'signin': 0.0003485535029627048, 'since': 0.0003485535029627048, 'sindu': 0.0003485535029627048, 'sir': 0.0005228302544440571, 'sis': 0.0003485535029627048, 'situation': 0.0003485535029627048, 'slice': 0.0003485535029627048, 'smarter': 0.0003485535029627048, 'smile': 0.0010456605088881143, 'smoke': 0.0003485535029627048, 'sms': 0.0003485535029627048, 'smth': 0.0003485535029627048, 'so': 0.0036598117811084, 'soft': 0.0003485535029627048, 'some': 0.0003485535029627048, 'someone': 0.0005228302544440571, 'something': 0.0006971070059254096, 'sometimes': 0.0003485535029627048, 'song': 0.0005228302544440571, 'soon': 0.0003485535029627048, 'sooner': 0.0003485535029627048, 'sorry': 0.001742767514813524, 'spanish': 0.0003485535029627048, 'speak': 0.0005228302544440571, 'special': 0.0005228302544440571, 'spell': 0.0005228302544440571, 'spend': 0.0003485535029627048, 'spent': 0.0003485535029627048, 'spoilt': 0.0003485535029627048, 'spoke': 0.0003485535029627048, 'staff': 0.0003485535029627048, 'stand': 0.0003485535029627048, 'started': 0.0003485535029627048, 'staying': 0.0003485535029627048, 'stays': 0.0003485535029627048, 'steed': 0.0003485535029627048, 'still': 0.0013942140118508191, 'stock': 0.0003485535029627048, 'stool': 0.0003485535029627048, 'stop': 0.0005228302544440571, 'story': 0.0003485535029627048, 'str': 0.0003485535029627048, 'stubborn': 0.0003485535029627048, 'studying': 0.0003485535029627048, 'stuff': 0.0005228302544440571, 'stuffmoro': 0.0003485535029627048, 'sucker': 0.0003485535029627048, 'suckers': 0.0003485535029627048, 'sucks': 0.0003485535029627048, 'suggest': 0.0003485535029627048, 'sum': 0.0005228302544440571, 'sunday': 0.0003485535029627048, 'sure': 0.0006971070059254096, 'surname': 0.0003485535029627048, 'sweet': 0.0003485535029627048, 'swt': 0.0003485535029627048, 'take': 0.0003485535029627048, 'taking': 0.0003485535029627048, 'talk': 0.0003485535029627048, 'tas': 0.0003485535029627048, 'tea': 0.0003485535029627048, 'tell': 0.000871383757406762, 'telling': 0.0003485535029627048, 'telugu': 0.0003485535029627048, 'test': 0.0005228302544440571, 'text': 0.000871383757406762, 'texting': 0.0003485535029627048, 'thank': 0.0003485535029627048, 'thanks': 0.0005228302544440571, 'that': 0.005402579295921924, 'thats': 0.0010456605088881143, 'the': 0.007145346810735448, 'their': 0.0003485535029627048, 'them': 0.0003485535029627048, 'then': 0.0013942140118508191, 'there': 0.0013942140118508191, 'theres': 0.0003485535029627048, 'they': 0.0003485535029627048, 'things': 0.000871383757406762, 'think': 0.0005228302544440571, 'thinked': 0.0003485535029627048, 'this': 0.0010456605088881143, 'thk': 0.0005228302544440571, 'tho': 0.0005228302544440571, 'those': 0.0003485535029627048, 'though': 0.0005228302544440571, 'thought': 0.0005228302544440571, 'tickets': 0.0003485535029627048, 'til': 0.0003485535029627048, 'till': 0.0003485535029627048, 'time': 0.001742767514813524, 'times': 0.0006971070059254096, 'timings': 0.0003485535029627048, 'tired': 0.0003485535029627048, 'tmorrowpls': 0.0003485535029627048, 'tmr': 0.0003485535029627048, 'to': 0.009410944579993029, 'today': 0.000871383757406762, 'toll': 0.0003485535029627048, 'tomo': 0.0005228302544440571, 'tomorrow': 0.0005228302544440571, 'tonight': 0.0006971070059254096, 'too': 0.0013942140118508191, 'took': 0.0003485535029627048, 'tortilla': 0.0003485535029627048, 'touch': 0.0003485535029627048, 'towards': 0.0003485535029627048, 'treat': 0.0006971070059254096, 'trouble': 0.0003485535029627048, 'truly': 0.0003485535029627048, 'trust': 0.0003485535029627048, 'try': 0.0005228302544440571, 'trying': 0.0003485535029627048, 'tt': 0.0003485535029627048, 'turn': 0.0003485535029627048, 'turns': 0.0003485535029627048, 'tv': 0.0003485535029627048, 'txt': 0.0003485535029627048, 'tyler': 0.0003485535029627048, 'type': 0.0003485535029627048, 'typical': 0.0003485535029627048, 'u': 0.005054025792959219, 'umma': 0.0005228302544440571, 'ummmawill': 0.0003485535029627048, 'uncle': 0.0003485535029627048, 'until': 0.0006971070059254096, 'up': 0.0013942140118508191, 'ur': 0.0013942140118508191, 'urgnt': 0.0003485535029627048, 'us': 0.0005228302544440571, 'used': 0.0003485535029627048, 'usf': 0.0003485535029627048, 'using': 0.0003485535029627048, 'usually': 0.0003485535029627048, 'utter': 0.0003485535029627048, 'v': 0.0012199372603694668, 'vaguely': 0.0003485535029627048, 'valuable': 0.0003485535029627048, 'vava': 0.0003485535029627048, 'very': 0.000871383757406762, 'vettam': 0.0006971070059254096, 'wa': 0.0003485535029627048, 'wah': 0.0003485535029627048, 'wait': 0.0003485535029627048, 'waiting': 0.0005228302544440571, 'wanna': 0.0003485535029627048, 'want': 0.000871383757406762, 'wanted': 0.0003485535029627048, 'was': 0.0012199372603694668, 'waste': 0.0003485535029627048, 'wat': 0.0005228302544440571, 'watching': 0.0005228302544440571, 'watts': 0.0003485535029627048, 'way': 0.0013942140118508191, 'we': 0.0012199372603694668, 'weak': 0.0005228302544440571, 'wed': 0.0003485535029627048, 'week': 0.0003485535029627048, 'weekend': 0.0005228302544440571, 'weighthaha': 0.0003485535029627048, 'well': 0.0006971070059254096, 'wen': 0.0005228302544440571, 'went': 0.0003485535029627048, 'were': 0.000871383757406762, 'wet': 0.0003485535029627048, 'what': 0.0013942140118508191, 'whats': 0.0003485535029627048, 'when': 0.0026141512722202857, 'where': 0.0005228302544440571, 'wheres': 0.0003485535029627048, 'which': 0.0005228302544440571, 'who': 0.0005228302544440571, 'whole': 0.0005228302544440571, 'why': 0.0005228302544440571, 'wif': 0.0006971070059254096, 'will': 0.0029627047751829907, 'windows': 0.0003485535029627048, 'wine': 0.0003485535029627048, 'wishes': 0.0003485535029627048, 'wishin': 0.0003485535029627048, 'wit': 0.0003485535029627048, 'with': 0.0027884280237016382, 'without': 0.0003485535029627048, 'wk': 0.0003485535029627048, 'wonderful': 0.0003485535029627048, 'wont': 0.0006971070059254096, 'words': 0.0003485535029627048, 'work': 0.0006971070059254096, 'working': 0.0003485535029627048, 'world': 0.0005228302544440571, 'worried': 0.0003485535029627048, 'worry': 0.0003485535029627048, 'worth': 0.0003485535029627048, 'would': 0.0003485535029627048, 'wow': 0.0005228302544440571, 'wun': 0.0003485535029627048, 'x': 0.0006971070059254096, 'xmas': 0.0003485535029627048, 'xuhui': 0.0003485535029627048, 'xx': 0.0005228302544440571, 'xxx': 0.0003485535029627048, 'y': 0.0005228302544440571, 'ya': 0.0005228302544440571, 'yeah': 0.0006971070059254096, 'year': 0.0003485535029627048, 'yes': 0.0013942140118508191, 'yesgauti': 0.0003485535029627048, 'yesterday': 0.0003485535029627048, 'yet': 0.0005228302544440571, 'yijuehotmailcom': 0.0003485535029627048, 'yo': 0.0003485535029627048, 'you': 0.01533635413035901, 'youd': 0.0003485535029627048, 'youhow': 0.0003485535029627048, 'youll': 0.0003485535029627048, 'your': 0.0038340885325897525, 'youre': 0.0005228302544440571, 'yourself': 0.0006971070059254096, 'youve': 0.0005228302544440571, 'yup': 0.0006971070059254096}\n",
      "------------------------------------------- \n",
      "\n",
      "Probability of words for \"question\" class \n",
      "\n",
      "{'': 0.4184360435007768, 'AJ': 0.0005178663904712584, 'Expires': 0.0005178663904712584, 'LDNWARW': 0.0005178663904712584, 'PPM': 0.0005178663904712584, 'SPTV': 0.0005178663904712584, 'SPTyrone': 0.0005178663904712584, 'a': 0.011910926980838944, 'about': 0.0010357327809425167, 'ac': 0.0010357327809425167, 'account': 0.0010357327809425167, 'acoentry': 0.0010357327809425167, 'advise': 0.0010357327809425167, 'again': 0.0010357327809425167, 'age': 0.0010357327809425167, 'algarve': 0.0010357327809425167, 'all': 0.0015535991714137752, 'am': 0.0010357327809425167, 'ampm': 0.0015535991714137752, 'and': 0.0031071983428275505, 'annoncement': 0.0010357327809425167, 'ansr': 0.0010357327809425167, 'any': 0.0010357327809425167, 'app': 0.0010357327809425167, 'apply': 0.0015535991714137752, 'are': 0.004142931123770067, 'arrange': 0.0010357327809425167, 'as': 0.002589331952356292, 'august': 0.0010357327809425167, 'award': 0.0010357327809425167, 'awarded': 0.0020714655618850335, 'back': 0.0010357327809425167, 'bangb': 0.0010357327809425167, 'bangbabes': 0.0010357327809425167, 'barbie': 0.0010357327809425167, 'be': 0.002589331952356292, 'been': 0.002589331952356292, 'between': 0.0015535991714137752, 'bonus': 0.0020714655618850335, 'bootydelious': 0.0010357327809425167, 'box': 0.0010357327809425167, 'boxwrc': 0.0010357327809425167, 'britney': 0.0010357327809425167, 'bt': 0.0010357327809425167, 'burns': 0.0010357327809425167, 'bxipwe': 0.0010357327809425167, 'by': 0.0010357327809425167, 'c': 0.0010357327809425167, 'call': 0.007767995857068877, 'caller': 0.0010357327809425167, 'camcorder': 0.0010357327809425167, 'camera': 0.0010357327809425167, 'cash': 0.002589331952356292, 'chances': 0.0010357327809425167, 'charged': 0.0015535991714137752, 'chat': 0.0010357327809425167, 'chgs': 0.0010357327809425167, 'cinema': 0.0010357327809425167, 'claim': 0.005696530295183843, 'click': 0.0015535991714137752, 'co': 0.0010357327809425167, 'code': 0.0031071983428275505, 'collected': 0.0010357327809425167, 'colour': 0.0010357327809425167, 'comes': 0.0010357327809425167, 'comp': 0.0010357327809425167, 'complimentary': 0.0010357327809425167, 'confirm': 0.0010357327809425167, 'congrats': 0.0010357327809425167, 'contact': 0.0020714655618850335, 'content': 0.0010357327809425167, 'correct': 0.0010357327809425167, 'cost': 0.0010357327809425167, 'country': 0.0010357327809425167, 'credit': 0.0010357327809425167, 'csh': 0.0010357327809425167, 'cup': 0.0010357327809425167, 'customer': 0.0031071983428275505, 'darling': 0.0010357327809425167, 'days': 0.0010357327809425167, 'delivery': 0.0020714655618850335, 'detroit': 0.0010357327809425167, 'devils': 0.0010357327809425167, 'did': 0.0010357327809425167, 'direct': 0.0010357327809425167, 'dis': 0.0010357327809425167, 'divorce': 0.0010357327809425167, 'do': 0.0010357327809425167, 'dogging': 0.0015535991714137752, 'dont': 0.0015535991714137752, 'download': 0.0010357327809425167, 'draw': 0.0015535991714137752, 'dvd': 0.0010357327809425167, 'eca': 0.0010357327809425167, 'eg': 0.0010357327809425167, 'end': 0.0020714655618850335, 'ending': 0.0010357327809425167, 'england': 0.0015535991714137752, 'enough': 0.0010357327809425167, 'entitled': 0.0010357327809425167, 'entry': 0.0015535991714137752, 'etc': 0.0010357327809425167, 'eurodisinc': 0.0010357327809425167, 'f': 0.0010357327809425167, 'fa': 0.0015535991714137752, 'female': 0.0010357327809425167, 'final': 0.0015535991714137752, 'find': 0.0010357327809425167, 'flights': 0.0015535991714137752, 'fml': 0.0010357327809425167, 'following': 0.0010357327809425167, 'for': 0.005178663904712584, 'free': 0.005178663904712584, 'freemsg': 0.0015535991714137752, 'freephone': 0.0010357327809425167, 'friend': 0.0010357327809425167, 'frnd': 0.0010357327809425167, 'from': 0.0031071983428275505, 'fun': 0.0010357327809425167, 'gent': 0.0010357327809425167, 'get': 0.0020714655618850335, 'go': 0.0010357327809425167, 'goalsteam': 0.0010357327809425167, 'goto': 0.0010357327809425167, 'gravel': 0.0010357327809425167, 'guaranteed': 0.002589331952356292, 'h': 0.0010357327809425167, 'had': 0.0010357327809425167, 'hardcore': 0.0010357327809425167, 'have': 0.005178663904712584, 'havent': 0.0010357327809425167, 'hear': 0.0015535991714137752, 'her': 0.0015535991714137752, 'here': 0.0010357327809425167, 'hey': 0.0010357327809425167, 'hl': 0.0010357327809425167, 'hockey': 0.0010357327809425167, 'holiday': 0.0015535991714137752, 'hours': 0.0010357327809425167, 'hrs': 0.0015535991714137752, 'httpwap': 0.0010357327809425167, 'i': 0.0010357327809425167, 'ice': 0.0010357327809425167, 'id': 0.0010357327809425167, 'identifier': 0.0010357327809425167, 'if': 0.0031071983428275505, 'im': 0.0010357327809425167, 'in': 0.002589331952356292, 'inc': 0.0015535991714137752, 'incorrect': 0.0010357327809425167, 'info': 0.0010357327809425167, 'internetservice': 0.0010357327809425167, 'inviting': 0.0010357327809425167, 'is': 0.0036250647332988087, 'it': 0.0015535991714137752, 'its': 0.0010357327809425167, 'jackpot': 0.0010357327809425167, 'jersey': 0.0010357327809425167, 'join': 0.0010357327809425167, 'k': 0.0010357327809425167, 'kens': 0.0010357327809425167, 'kl': 0.0010357327809425167, 'know': 0.0010357327809425167, 'laid': 0.0010357327809425167, 'landline': 0.0010357327809425167, 'largest': 0.0010357327809425167, 'last': 0.0015535991714137752, 'latest': 0.0010357327809425167, 'lccltd': 0.0010357327809425167, 'like': 0.0010357327809425167, 'link': 0.0010357327809425167, 'live': 0.0020714655618850335, 'local': 0.0010357327809425167, 'locations': 0.0010357327809425167, 'ls': 0.0010357327809425167, 'ltd': 0.0010357327809425167, 'luv': 0.0010357327809425167, 'macedonia': 0.0010357327809425167, 'match': 0.0010357327809425167, 'matrix': 0.0010357327809425167, 'may': 0.0010357327809425167, 'membership': 0.0010357327809425167, 'menu': 0.0010357327809425167, 'message': 0.0010357327809425167, 'miss': 0.0015535991714137752, 'mix': 0.0010357327809425167, 'mk': 0.0010357327809425167, 'mob': 0.0015535991714137752, 'mobile': 0.0036250647332988087, 'mobiles': 0.0010357327809425167, 'month': 0.0010357327809425167, 'months': 0.0010357327809425167, 'more': 0.0015535991714137752, 'morefrmmob': 0.0010357327809425167, 'msg': 0.0020714655618850335, 'msgs': 0.0010357327809425167, 'must': 0.0010357327809425167, 'my': 0.0010357327809425167, 'national': 0.0010357327809425167, 'netcollex': 0.0010357327809425167, 'network': 0.0020714655618850335, 'new': 0.0020714655618850335, 'news': 0.0010357327809425167, 'next': 0.0010357327809425167, 'no': 0.004142931123770067, 'nokia': 0.0010357327809425167, 'not': 0.0020714655618850335, 'nothing': 0.0010357327809425167, 'now': 0.002589331952356292, 'nt': 0.0010357327809425167, 'numbers': 0.0010357327809425167, 'of': 0.0015535991714137752, 'ok': 0.0010357327809425167, 'on': 0.0036250647332988087, 'only': 0.0020714655618850335, 'operator': 0.0015535991714137752, 'or': 0.005696530295183843, 'order': 0.0010357327809425167, 'our': 0.0031071983428275505, 'out': 0.0015535991714137752, 'overs': 0.0010357327809425167, 'p': 0.0010357327809425167, 'pass': 0.0010357327809425167, 'password': 0.0010357327809425167, 'pday': 0.0010357327809425167, 'per': 0.0010357327809425167, 'play': 0.0010357327809425167, 'player': 0.0010357327809425167, 'please': 0.0036250647332988087, 'pleased': 0.0010357327809425167, 'pls': 0.0010357327809425167, 'pm': 0.0010357327809425167, 'pmin': 0.0010357327809425167, 'pmsgp': 0.0010357327809425167, 'po': 0.0010357327809425167, 'pobox': 0.0010357327809425167, 'poboxoxwwq': 0.0010357327809425167, 'points': 0.0010357327809425167, 'pounds': 0.0010357327809425167, 'ppm': 0.0015535991714137752, 'ppw': 0.0010357327809425167, 'private': 0.0010357327809425167, 'prize': 0.004660797514241326, 'q': 0.0010357327809425167, 'questionstd': 0.0010357327809425167, 'quiz': 0.0010357327809425167, 'quoting': 0.0010357327809425167, 'r': 0.0010357327809425167, 'randy': 0.0010357327809425167, 'rates': 0.0010357327809425167, 'ratetcs': 0.0010357327809425167, 'rcv': 0.0015535991714137752, 're': 0.0010357327809425167, 'real': 0.0010357327809425167, 'receive': 0.0031071983428275505, 'receivea': 0.0010357327809425167, 'recent': 0.0010357327809425167, 'red': 0.0010357327809425167, 'replied': 0.0010357327809425167, 'reply': 0.0036250647332988087, 'replying': 0.0010357327809425167, 'representative': 0.0015535991714137752, 'review': 0.0010357327809425167, 'reward': 0.0010357327809425167, 'ringtone': 0.0015535991714137752, 'rodger': 0.0010357327809425167, 's': 0.0010357327809425167, 'scotland': 0.0010357327809425167, 'see': 0.0010357327809425167, 'selected': 0.002589331952356292, 'send': 0.0020714655618850335, 'sent': 0.0010357327809425167, 'service': 0.002589331952356292, 'services': 0.0010357327809425167, 'sexy': 0.0010357327809425167, 'should': 0.0010357327809425167, 'shows': 0.0020714655618850335, 'shracomorsglsuplt': 0.0010357327809425167, 'simply': 0.0010357327809425167, 'six': 0.0010357327809425167, 'sms': 0.0015535991714137752, 'some': 0.0010357327809425167, 'sony': 0.0010357327809425167, 'speak': 0.0015535991714137752, 'special': 0.0010357327809425167, 'specially': 0.0015535991714137752, 'sptv': 0.0015535991714137752, 'st': 0.0010357327809425167, 'standard': 0.0010357327809425167, 'starwars': 0.0010357327809425167, 'statement': 0.0010357327809425167, 'std': 0.0010357327809425167, 'still': 0.0010357327809425167, 'stop': 0.002589331952356292, 'stuff': 0.0010357327809425167, 'subscription': 0.0010357327809425167, 'sunshine': 0.0010357327809425167, 'suprman': 0.0010357327809425167, 'svc': 0.0010357327809425167, 'tb': 0.0010357327809425167, 'tc': 0.0010357327809425167, 'team': 0.0010357327809425167, 'text': 0.002589331952356292, 'th': 0.0010357327809425167, 'thanks': 0.0010357327809425167, 'that': 0.0020714655618850335, 'the': 0.007767995857068877, 'there': 0.0010357327809425167, 'this': 0.0010357327809425167, 'tkts': 0.0010357327809425167, 'to': 0.022268254790264112, 'todays': 0.0010357327809425167, 'tomorrow': 0.0010357327809425167, 'tonight': 0.0010357327809425167, 'top': 0.0010357327809425167, 'trav': 0.0010357327809425167, 'tried': 0.0010357327809425167, 'trip': 0.0010357327809425167, 'try': 0.0015535991714137752, 'trying': 0.0015535991714137752, 'trywales': 0.0010357327809425167, 'tsandcs': 0.0010357327809425167, 'tv': 0.0010357327809425167, 'txt': 0.004660797514241326, 'txting': 0.0010357327809425167, 'u': 0.005696530295183843, 'uk': 0.0010357327809425167, 'uks': 0.0010357327809425167, 'unique': 0.0010357327809425167, 'unredeemed': 0.0010357327809425167, 'up': 0.0010357327809425167, 'update': 0.0015535991714137752, 'ur': 0.0036250647332988087, 'urgent': 0.002589331952356292, 'use': 0.0010357327809425167, 'usher': 0.0010357327809425167, 'v': 0.0015535991714137752, 'valid': 0.0020714655618850335, 'valued': 0.0015535991714137752, 'verify': 0.0015535991714137752, 'voda': 0.0010357327809425167, 'waiting': 0.0015535991714137752, 'want': 0.0015535991714137752, 'wap': 0.0015535991714137752, 'was': 0.0010357327809425167, 'way': 0.0010357327809425167, 'we': 0.0020714655618850335, 'week': 0.0010357327809425167, 'weekends': 0.0015535991714137752, 'weeks': 0.0010357327809425167, 'which': 0.0010357327809425167, 'why': 0.0010357327809425167, 'will': 0.0015535991714137752, 'win': 0.0020714655618850335, 'wings': 0.0010357327809425167, 'winner': 0.0020714655618850335, 'with': 0.002589331952356292, 'wkly': 0.0015535991714137752, 'won': 0.0031071983428275505, 'word': 0.0015535991714137752, 'wwwareyouuniquecouk': 0.0010357327809425167, 'wwwdbuknet': 0.0010357327809425167, 'wwwsmsacubootydelious': 0.0010357327809425167, 'xxx': 0.0010357327809425167, 'xxxmobilemovieclub': 0.0010357327809425167, 'xxxmobilemovieclubcomnQJKGIGHJJGCBL': 0.0005178663904712584, 'year': 0.0010357327809425167, 'years': 0.0010357327809425167, 'yes': 0.0015535991714137752, 'you': 0.012428793371310202, 'youll': 0.0010357327809425167, 'your': 0.005178663904712584, 'yours': 0.0010357327809425167, 'yr': 0.0010357327809425167}\n"
     ]
    }
   ],
   "source": [
    "# Get word probabilities for statement class\n",
    "a = 1\n",
    "prob_ham = []\n",
    "for count in freq_list_ham:\n",
    "    #print(word, count)\n",
    "    prob_ham.append((count+a)/(sum(freq_list_ham)+len(freq_list_ham)*a))\n",
    "prob_ham.append(a/(sum(freq_list_ham)+len(freq_list_ham)*a))\n",
    "    \n",
    "# Get word probabilities for question class\n",
    "\n",
    "prob_spam = []\n",
    "for count in freq_list_spam:\n",
    "    prob_spam.append((count+a)/(sum(freq_list_spam)+len(freq_list_spam)*a))\n",
    "prob_spam.append(a/(sum(freq_list_spam)+len(freq_list_spam)*a))   \n",
    "    \n",
    "    \n",
    "print('Probability of words for \"statement\" class \\n')\n",
    "print(dict(zip(word_list_ham, prob_ham)))\n",
    "print('------------------------------------------- \\n')\n",
    "print('Probability of words for \"question\" class \\n')\n",
    "print(dict(zip(word_list_spam, prob_spam)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(className):    \n",
    "    denominator = len(ham_docs) + len(spam_docs)\n",
    "    \n",
    "    if className == 'ham':\n",
    "        numerator =  len(ham_docs)\n",
    "    else:\n",
    "        numerator =  len(spam_docs)\n",
    "        \n",
    "    return np.divide(numerator,denominator)\n",
    "    \n",
    "# Calculate class conditional probability for a sentence\n",
    "    \n",
    "def classCondProb(sentence, className):\n",
    "    words = get_words(sentence)\n",
    "    prob = 1\n",
    "    for word in words:\n",
    "        if className == 'ham':\n",
    "            if word not in word_list_ham:\n",
    "                # print('hi')\n",
    "                prob = prob * prior('ham')\n",
    "            else:\n",
    "                idx = np.where(word_list_ham == word)\n",
    "                prob = prob * prob_ham[np.array(idx)[0,0]]\n",
    "        else:\n",
    "            if word not in word_list_spam:\n",
    "                prob = prob * prior('spam')\n",
    "            else:\n",
    "                idx = np.where(word_list_spam == word)\n",
    "                # print(idx)\n",
    "                prob = prob * prob_spam[np.array(idx)[0,0]]   \n",
    "    \n",
    "    return prob\n",
    "\n",
    "# Predict class of a sentence\n",
    "\n",
    "def predict(sentence):\n",
    "    prob_statement = classCondProb(sentence, 'ham') * prior('ham')\n",
    "    prob_question = classCondProb(sentence, 'spam') * prior('spam')\n",
    "    if  prob_statement > prob_question:\n",
    "        return 'ham'\n",
    "    else:\n",
    "        return 'spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting prediction for Found it ENC  ltgt  where you at\"\n",
      "ham\n",
      "Getting prediction for I sent you  ltgt  bucks\"\n",
      "spam\n",
      "Getting prediction for Hello darlin ive finished college now so txt me when u finish if u can love Kate xxx\"\n",
      "ham\n",
      "Getting prediction for Your account has been refilled successfully by INR  ltDECIMALgt  Your KeralaCircle prepaid account balance is Rs  ltDECIMALgt  Your Transaction ID is KR ltgt \"\n",
      "ham\n",
      "Getting prediction for Goodmorning sleeping ga\"\n",
      "ham\n",
      "Getting prediction for U call me alter at  ok\"\n",
      "spam\n",
      "Getting prediction for  say until like dat i dun buy ericsson oso cannot oredi lar\"\n",
      "spam\n",
      "Getting prediction for As I entered my cabin my PA said  Happy Bday Boss  I felt special She askd me  lunch After lunch she invited me to her apartment We went there\"\n",
      "ham\n",
      "Getting prediction for Aight yo dats straight dogg\"\n",
      "ham\n",
      "Getting prediction for You please give us connection today itself before  ltDECIMALgt  or refund the bill\"\n",
      "ham\n",
      "Getting prediction for Both  i shoot big loads so get ready\"\n",
      "ham\n",
      "Getting prediction for Whats up bruv hope you had a great break Do have a rewarding semester\"\n",
      "spam\n",
      "Getting prediction for Home so we can always chat\"\n",
      "spam\n",
      "Getting prediction for Kkgoodstudy well\"\n",
      "ham\n",
      "Getting prediction for Yup How  noe leh\"\n",
      "ham\n",
      "Getting prediction for Sounds great Are you home now\"\n",
      "spam\n",
      "Getting prediction for Finally the match heading towards draw as your prediction\"\n",
      "ham\n",
      "Getting prediction for Tired I havent slept well the past few nights\"\n",
      "ham\n",
      "Getting prediction for Easy ahsen got selected means its good\"\n",
      "spam\n",
      "Getting prediction for I have to take exam with march \"\n",
      "spam\n"
     ]
    }
   ],
   "source": [
    "test_docs = list([test['Message'] for index,test in test_data.iterrows()])\n",
    "\n",
    "result_hat  = []\n",
    "for sentence in test_docs[:20]:\n",
    "    print('Getting prediction for %s\"' % sentence)\n",
    "    print(predict(sentence))\n",
    "for sentence in test_docs:\n",
    "    result_hat.append(predict(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.Category,result_hat\n",
    "acc = np.where(test_data.Category == result_hat)[0].shape[0]/len(test_data) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy percent = 60.0\n"
     ]
    }
   ],
   "source": [
    "print(f' Accuracy percent = {acc}')"
   ]
  },
 
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
